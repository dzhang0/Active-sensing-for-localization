{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NYR5rv3MFeX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#Constructing the array responses for AoA candidates\n",
        "\"\"\"\n",
        "************** Input\n",
        "phi_min: Lower-bound of AoAs\n",
        "phi_max: Upper-bound of AoAs\n",
        "N: # Antennas\n",
        "delta_inv: # Coarse AoA Candidates (Intervals)\n",
        "delta_inv_OS: # AoA Candidates (After over-sampling)\n",
        "************** Ouput \n",
        "phi: Coarse AoA Candidates   \n",
        "A_BS: Collection of array responses for Coarse AoA Candidates\n",
        "phi: AoA Candidates   \n",
        "A_BS: Collection of array responses for AoA Candidates\n",
        "\"\"\"\n",
        "def func_codedesign_cont(delta_inv,delta_inv_OS,phi_min,phi_max,N):\n",
        "    delta_theta = (phi_max-phi_min)/delta_inv;\n",
        "    phi = np.linspace(start=phi_min+delta_theta/2,stop=phi_max-delta_theta/2,num=delta_inv) \n",
        "    from0toN = np.float32(list(range(0, N)))\n",
        "    A_BS = np.zeros([N,delta_inv],dtype=np.complex64)\n",
        "    for i in range(delta_inv):\n",
        "        a_phi = np.exp(1j*np.pi*from0toN*np.sin(phi[i]))\n",
        "        A_BS[:,i] = np.transpose(a_phi)\n",
        "\n",
        "    delta_theta = (phi_max-phi_min)/delta_inv_OS\n",
        "    phi_OS = np.linspace(start=phi_min+delta_theta/2,stop=phi_max-delta_theta/2,num=delta_inv_OS)  \n",
        "    A_BS_OS = np.zeros([N,delta_inv_OS],dtype=np.complex64)\n",
        "    for i in range(delta_inv_OS):\n",
        "        a_phi = np.exp(1j*np.pi*from0toN*np.sin(phi_OS[i]))\n",
        "        A_BS_OS[:,i] = np.transpose(a_phi)\n",
        "        \n",
        "    return A_BS, phi, A_BS_OS, phi_OS   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWhpygUtMMrc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def generate_location(num_users):\n",
        "    location_user = np.empty([num_users, 3])\n",
        "    for k in range(num_users):\n",
        "        #x = np.random.uniform(5, 55) \n",
        "        #y = np.random.uniform(-35, 35)\n",
        "        x = np.random.uniform(5, 35) \n",
        "        y = np.random.uniform(-35, 35)\n",
        "        z = -20\n",
        "        coordinate_k = np.array([x, y, z])\n",
        "        location_user[k, :] = coordinate_k\n",
        "    return location_user\n",
        "\n",
        "def generate_location_2D(num_users):\n",
        "    location_user = np.empty([num_users, 2])\n",
        "    for k in range(num_users):\n",
        "        x = np.random.uniform(5, 35) \n",
        "        y = np.random.uniform(-35, 35)\n",
        "        coordinate_k = np.array([x, y])\n",
        "        location_user[k, :] = coordinate_k\n",
        "    return location_user\n",
        "\n",
        "\n",
        "\n",
        "def path_loss_r(d):\n",
        "    loss = 30 + 22.0 * np.log10(d)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def path_loss_d(d):\n",
        "    loss = 32.6 + 36.7 * np.log10(d)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def generate_pathloss_aoa_aod(location_user, location_bs, location_irs):\n",
        "    \"\"\"\n",
        "    :param location_user: array (num_user,2)\n",
        "    :param location_bs: array (2,)\n",
        "    :param location_irs: array (2,)\n",
        "    :return: pathloss = (pathloss_irs_bs, pathloss_irs_user, pathloss_bs_user)\n",
        "            cos_phi = (cos_phi_1, cos_phi_2, cos_phi_3)\n",
        "    \"\"\"\n",
        "    num_user = location_user.shape[0]\n",
        "    # ========bs-irs==============\n",
        "    d0 = np.linalg.norm(location_bs - location_irs)\n",
        "    pathloss_irs_bs = path_loss_r(d0)\n",
        "    aoa_bs = ( location_irs[0] - location_bs[0]) / d0\n",
        "    aod_irs_y = (location_bs[1]-location_irs[1]) / d0\n",
        "    aod_irs_z = (location_bs[2]-location_irs[2]) / d0\n",
        "    # =========irs-user=============\n",
        "    pathloss_irs_user = []\n",
        "    aoa_irs_y = []\n",
        "    aoa_irs_z = []\n",
        "    for k in range(num_user):\n",
        "        d_k = np.linalg.norm(location_user[k] - location_irs)\n",
        "        pathloss_irs_user.append(path_loss_r(d_k))\n",
        "        aoa_irs_y_k = (location_user[k][1] - location_irs[1]) / d_k\n",
        "        aoa_irs_z_k = (location_user[k][2] - location_irs[2]) / d_k\n",
        "        aoa_irs_y.append(aoa_irs_y_k)\n",
        "        aoa_irs_z.append(aoa_irs_z_k)\n",
        "    aoa_irs_y = np.array(aoa_irs_y)\n",
        "    aoa_irs_z = np.array(aoa_irs_z)\n",
        "\n",
        "    # =========bs-user=============\n",
        "    pathloss_bs_user = np.zeros([num_user, 1])\n",
        "    for k in range(num_user):\n",
        "        d_k = np.linalg.norm(location_user[k] - location_bs)\n",
        "        pathloss_bs_user_k = path_loss_d(d_k)\n",
        "        pathloss_bs_user[k, :] = pathloss_bs_user_k\n",
        "\n",
        "    pathloss = (pathloss_irs_bs, np.array(pathloss_irs_user), np.array(pathloss_bs_user))\n",
        "    aoa_aod = (aoa_bs, aod_irs_y, aod_irs_z, aoa_irs_y, aoa_irs_z)\n",
        "    return pathloss, aoa_aod\n",
        "\n",
        "def generate_pathloss_aoa_aod_fullRician(location_user, location_bs, location_irs):\n",
        "    \"\"\"\n",
        "    :param location_user: array (num_user,2)\n",
        "    :param location_bs: array (2,)\n",
        "    :param location_irs: array (2,)\n",
        "    :return: pathloss = (pathloss_irs_bs, pathloss_irs_user, pathloss_bs_user)\n",
        "            cos_phi = (cos_phi_1, cos_phi_2, cos_phi_3)\n",
        "    \"\"\"\n",
        "    num_user = location_user.shape[0]\n",
        "    # ========bs-irs==============\n",
        "    d0 = np.linalg.norm(location_bs - location_irs)\n",
        "    pathloss_irs_bs = path_loss_r(d0)\n",
        "    aoa_bs = ( location_irs[0] - location_bs[0]) / d0\n",
        "    aod_irs_y = (location_bs[1]-location_irs[1]) / d0\n",
        "    aod_irs_z = (location_bs[2]-location_irs[2]) / d0\n",
        "    # =========irs-user=============\n",
        "    pathloss_irs_user = []\n",
        "    aoa_irs_y = []\n",
        "    aoa_irs_z = []\n",
        "    for k in range(num_user):\n",
        "        d_k = np.linalg.norm(location_user[k] - location_irs)\n",
        "        pathloss_irs_user.append(path_loss_r(d_k))\n",
        "        aoa_irs_y_k = (location_user[k][1] - location_irs[1]) / d_k\n",
        "        aoa_irs_z_k = (location_user[k][2] - location_irs[2]) / d_k\n",
        "        aoa_irs_y.append(aoa_irs_y_k)\n",
        "        aoa_irs_z.append(aoa_irs_z_k)\n",
        "    aoa_irs_y = np.array(aoa_irs_y)\n",
        "    aoa_irs_z = np.array(aoa_irs_z)\n",
        "\n",
        "    # =========bs-user=============\n",
        "    aoa_bs_y = []\n",
        "    aoa_bs_z = []\n",
        "    pathloss_bs_user = np.zeros([num_user, 1])\n",
        "    for k in range(num_user):\n",
        "        d_k = np.linalg.norm(location_user[k] - location_bs)\n",
        "        pathloss_bs_user_k = path_loss_d(d_k)\n",
        "        pathloss_bs_user[k, :] = pathloss_bs_user_k\n",
        "        aod_bs_k_y = (location_bs[1]-location_irs[1]) / d_k\n",
        "        aod_bs_k_z = (location_bs[2]-location_irs[2]) / d_k\n",
        "        aoa_bs_y.append(aod_bs_k_y)\n",
        "        aoa_bs_z.append(aod_bs_k_z)\n",
        "    aoa_bs_y = np.array(aoa_bs_y)\n",
        "    aoa_bs_z = np.array(aoa_bs_z)\n",
        "\n",
        "    pathloss = (pathloss_irs_bs, np.array(pathloss_irs_user), np.array(pathloss_bs_user))\n",
        "    aoa_aod = (aoa_bs, aod_irs_y, aod_irs_z, aoa_irs_y, aoa_irs_z, aoa_bs_y , aoa_bs_z)\n",
        "    return pathloss, aoa_aod\n",
        "\n",
        "def generate_pathloss_aoa_aod_2D(location_user, location_bs, location_irs):\n",
        "    \"\"\"\n",
        "    :param location_user: array (num_user,2)\n",
        "    :param location_bs: array (2,)\n",
        "    :param location_irs: array (2,)\n",
        "    :return: pathloss = (pathloss_irs_bs, pathloss_irs_user, pathloss_bs_user)\n",
        "            cos_phi = (cos_phi_1, cos_phi_2, cos_phi_3)\n",
        "    \"\"\"\n",
        "    num_user = location_user.shape[0]\n",
        "    # ========bs-irs==============\n",
        "    d0 = np.linalg.norm(location_bs - location_irs)\n",
        "    pathloss_irs_bs = path_loss_r(d0)\n",
        "    aoa_bs = ( location_irs[0] - location_bs[0]) / d0\n",
        "    aod_irs_y = (location_bs[1]-location_irs[1]) / d0\n",
        "    #aod_irs_z = (location_bs[2]-location_irs[2]) / d0\n",
        "    # =========irs-user=============\n",
        "    pathloss_irs_user = []\n",
        "    aoa_irs_y = []\n",
        "    #aoa_irs_z = []\n",
        "    for k in range(num_user):\n",
        "        d_k = np.linalg.norm(location_user[k] - location_irs)\n",
        "        pathloss_irs_user.append(path_loss_r(d_k))\n",
        "        aoa_irs_y_k = (location_user[k][1] - location_irs[1]) / d_k\n",
        "        #aoa_irs_z_k = (location_user[k][2] - location_irs[2]) / d_k\n",
        "        aoa_irs_y.append(aoa_irs_y_k)\n",
        "        #aoa_irs_z.append(aoa_irs_z_k)\n",
        "    aoa_irs_y = np.array(aoa_irs_y)\n",
        "    #aoa_irs_z = np.array(aoa_irs_z)\n",
        "\n",
        "    # =========bs-user=============\n",
        "    pathloss_bs_user = np.zeros([num_user, 1])\n",
        "    for k in range(num_user):\n",
        "        d_k = np.linalg.norm(location_user[k] - location_bs)\n",
        "        pathloss_bs_user_k = path_loss_d(d_k)\n",
        "        pathloss_bs_user[k, :] = pathloss_bs_user_k\n",
        "\n",
        "    pathloss = (pathloss_irs_bs, np.array(pathloss_irs_user), np.array(pathloss_bs_user))\n",
        "    aoa_aod = (aoa_bs, aod_irs_y, aoa_irs_y)\n",
        "    return pathloss, aoa_aod\n",
        "\n",
        "######################################################################################################################## distance not halfed\n",
        "def generate_channel(params_system, location_bs=np.array([100, -100, 0]), location_irs=np.array([0, 0, 0]),\n",
        "                     location_user_initial=None, Rician_factor=10, scale_factor=100, num_samples=100,irs_Nh = 16):\n",
        "    # scale_factor: can be viewed as (downlink noise_power_dB- downlink Pt)\n",
        "\n",
        "    (num_antenna_bs, num_elements_irs, num_user) = params_system\n",
        "\n",
        "    channel_bs_irs, channel_bs_user, channel_irs_user, set_location_user = [], [], [], []\n",
        "    for ii in range(num_samples):\n",
        "        if location_user_initial is None:\n",
        "            location_user = generate_location(num_user)\n",
        "            set_location_user.append(location_user)\n",
        "        else:\n",
        "            location_user = location_user_initial\n",
        "            set_location_user.append(location_user)\n",
        "\n",
        "        pathloss, aoa_aod = generate_pathloss_aoa_aod(location_user, location_bs, location_irs)\n",
        "        (pathloss_irs_bs, pathloss_irs_user, pathloss_bs_user) = pathloss\n",
        "        (aoa_bs, aod_irs_y, aod_irs_z, aoa_irs_y, aoa_irs_z) = aoa_aod\n",
        "\n",
        "        pathloss_bs_user = pathloss_bs_user - scale_factor\n",
        "        pathloss_irs_bs = pathloss_irs_bs - scale_factor / 2\n",
        "        pathloss_irs_user = pathloss_irs_user - scale_factor / 2\n",
        "        pathloss_bs_user = np.sqrt(10 ** ((-pathloss_bs_user) / 10))\n",
        "        pathloss_irs_user = np.sqrt(10 ** ((-pathloss_irs_user) / 10))\n",
        "        pathloss_irs_bs = np.sqrt(10 ** ((-pathloss_irs_bs) / 10))\n",
        "\n",
        "        # tmp:(num_antenna_bs,num_user) channel between BS and user\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_user]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_user])\n",
        "        tmp = tmp * pathloss_bs_user.reshape(1, num_user)\n",
        "\n",
        "        # ######################################################################\n",
        "        # # some user have LOS, some does not have LOS, make hd equal to 0.\n",
        "        \n",
        "        # user_no_los = random.sample(range(0, num_user), int(np.floor(num_user/3)))\n",
        "        # tmp[:,user_no_los] = np.zeros([num_antenna_bs,1])\n",
        "        # #tmp[:,user_no_los] = [0.0000000001]\n",
        "        \n",
        "        # ######################################################################\n",
        "        channel_bs_user.append(tmp)\n",
        "\n",
        "        # tmp: (num_antenna_bs,num_elements_irs) channel between IRS and BS\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_elements_irs]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_elements_irs])\n",
        "        a_bs = np.exp(1j * np.pi * aoa_bs * np.arange(num_antenna_bs))\n",
        "        a_bs = np.reshape(a_bs, [num_antenna_bs, 1])\n",
        "\n",
        "        i1 = np.mod(np.arange(num_elements_irs),irs_Nh)\n",
        "        i2 = np.floor(np.arange(num_elements_irs)/irs_Nh)\n",
        "        a_irs_bs = np.exp(1j * np.pi * (i1*aod_irs_y+i2*aod_irs_z))\n",
        "        a_irs_bs =  np.reshape(a_irs_bs, [num_elements_irs, 1])\n",
        "        los_irs_bs = a_bs @ np.transpose(a_irs_bs.conjugate())\n",
        "        tmp = np.sqrt(Rician_factor / (1 + Rician_factor)) * los_irs_bs + np.sqrt(1/(1 + Rician_factor)) * tmp\n",
        "        tmp = tmp * pathloss_irs_bs\n",
        "        channel_bs_irs.append(tmp)\n",
        "\n",
        "        # tmp:(num_elements_irs,num_user) channel between IRS and user\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_elements_irs, num_user]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_elements_irs, num_user])\n",
        "        for k in range(num_user):\n",
        "            a_irs_user = np.exp(1j * np.pi * (i1 * aoa_irs_y[k] + i2 * aoa_irs_z[k]))\n",
        "            tmp[:, k] = np.sqrt(Rician_factor/(1+Rician_factor))*a_irs_user+np.sqrt(1/(1+Rician_factor))*tmp[:, k]\n",
        "            tmp[:, k] = tmp[:, k] * pathloss_irs_user[k]\n",
        "        channel_irs_user.append(tmp)\n",
        "    channels = (np.array(channel_bs_user), np.array(channel_irs_user), np.array(channel_bs_irs))\n",
        "    return channels, set_location_user\n",
        "\n",
        "def generate_channel_fullRician(params_system, location_bs=np.array([100, -100, 0]), location_irs=np.array([0, 0, 0]),\n",
        "                     location_user_initial=None, Rician_factor=10, scale_factor=100, num_samples=100,irs_Nh = 16):\n",
        "    # scale_factor: can be viewed as (downlink noise_power_dB- downlink Pt)\n",
        "\n",
        "    (num_antenna_bs, num_elements_irs, num_user) = params_system\n",
        "\n",
        "    channel_bs_irs, channel_bs_user, channel_irs_user, set_location_user = [], [], [], []\n",
        "    for ii in range(num_samples):\n",
        "        if location_user_initial is None:\n",
        "            location_user = generate_location(num_user)\n",
        "            set_location_user.append(location_user)\n",
        "        else:\n",
        "            location_user = location_user_initial\n",
        "            set_location_user.append(location_user)\n",
        "\n",
        "        pathloss, aoa_aod = generate_pathloss_aoa_aod_fullRician(location_user, location_bs, location_irs)\n",
        "        (pathloss_irs_bs, pathloss_irs_user, pathloss_bs_user) = pathloss\n",
        "        (aoa_bs, aod_irs_y, aod_irs_z, aoa_irs_y, aoa_irs_z, aoa_bs_y, aoa_bs_z) = aoa_aod\n",
        "\n",
        "        pathloss_bs_user = pathloss_bs_user - scale_factor\n",
        "        pathloss_irs_bs = pathloss_irs_bs - scale_factor / 2\n",
        "        pathloss_irs_user = pathloss_irs_user - scale_factor / 2\n",
        "        pathloss_bs_user = np.sqrt(10 ** ((-pathloss_bs_user) / 10))\n",
        "        pathloss_irs_user = np.sqrt(10 ** ((-pathloss_irs_user) / 10))\n",
        "        pathloss_irs_bs = np.sqrt(10 ** ((-pathloss_irs_bs) / 10))\n",
        "\n",
        "        # tmp:(num_antenna_bs,num_user) channel between BS and user\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_user]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_user])\n",
        "        i1 = np.arange(1)\n",
        "        for k in range(num_user):\n",
        "            a_bs_user = np.exp(1j * np.pi * 0)\n",
        "            tmp[:, k] = np.sqrt(Rician_factor/(1+Rician_factor))*a_bs_user+np.sqrt(1/(1+Rician_factor))*tmp[:, k]\n",
        "            tmp[:, k] = tmp[:, k] * pathloss_bs_user[k]\n",
        " \n",
        "        channel_bs_user.append(tmp)\n",
        "\n",
        "        #################        channel between IRS and BS\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_elements_irs]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_elements_irs])\n",
        "        a_bs = np.exp(1j * np.pi * aoa_bs * np.arange(num_antenna_bs))\n",
        "        a_bs = np.reshape(a_bs, [num_antenna_bs, 1])\n",
        "\n",
        "        i1 = np.mod(np.arange(num_elements_irs),irs_Nh)\n",
        "        i2 = np.floor(np.arange(num_elements_irs)/irs_Nh)\n",
        "        a_irs_bs = np.exp(1j * np.pi * (i1*aod_irs_y+i2*aod_irs_z))\n",
        "        a_irs_bs =  np.reshape(a_irs_bs, [num_elements_irs, 1])\n",
        "        los_irs_bs = a_bs @ np.transpose(a_irs_bs.conjugate())\n",
        "        tmp = np.sqrt(Rician_factor / (1 + Rician_factor)) * los_irs_bs + np.sqrt(1/(1 + Rician_factor)) * tmp\n",
        "        tmp = tmp * pathloss_irs_bs\n",
        "        channel_bs_irs.append(tmp)\n",
        "\n",
        "        ###############         channel between IRS and user\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_elements_irs, num_user]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_elements_irs, num_user])\n",
        "        for k in range(num_user):\n",
        "            a_irs_user = np.exp(1j * np.pi * (i1 * aoa_irs_y[k] + i2 * aoa_irs_z[k]))\n",
        "            tmp[:, k] = np.sqrt(Rician_factor/(1+Rician_factor))*a_irs_user+np.sqrt(1/(1+Rician_factor))*tmp[:, k]\n",
        "            tmp[:, k] = tmp[:, k] * pathloss_irs_user[k]\n",
        "        channel_irs_user.append(tmp)\n",
        "    channels = (np.array(channel_bs_user), np.array(channel_irs_user), np.array(channel_bs_irs))\n",
        "    return channels, set_location_user\n",
        "\n",
        "\n",
        "def generate_channel_2D(params_system, location_bs=np.array([100, -100]), location_irs=np.array([0, 0]),\n",
        "                     location_user_initial=None, Rician_factor=10, scale_factor=100, num_samples=100,irs_Nh = 16):\n",
        "\n",
        "    (num_antenna_bs, num_elements_irs, num_user) = params_system\n",
        "\n",
        "    channel_bs_irs, channel_bs_user, channel_irs_user, set_location_user = [], [], [], []\n",
        "    aoa_irs_y_set = []\n",
        "    pathloss_irs_user_set = []\n",
        "    for ii in range(num_samples):\n",
        "        if location_user_initial is None:\n",
        "            location_user = generate_location_2D(num_user)\n",
        "            set_location_user.append(location_user)\n",
        "        else:\n",
        "            location_user = location_user_initial\n",
        "            set_location_user.append(location_user)\n",
        "\n",
        "        pathloss, aoa_aod = generate_pathloss_aoa_aod_2D(location_user, location_bs, location_irs)\n",
        "        (pathloss_irs_bs, pathloss_irs_user, pathloss_bs_user) = pathloss\n",
        "        (aoa_bs, aod_irs_y, aoa_irs_y) = aoa_aod\n",
        "\n",
        "        pathloss_bs_user = pathloss_bs_user - scale_factor\n",
        "        pathloss_irs_bs = pathloss_irs_bs - scale_factor / 2\n",
        "        pathloss_irs_user = pathloss_irs_user - scale_factor / 2\n",
        "        pathloss_bs_user = np.sqrt(10 ** ((-pathloss_bs_user) / 10))\n",
        "        pathloss_irs_user = np.sqrt(10 ** ((-pathloss_irs_user) / 10))\n",
        "        pathloss_irs_bs = np.sqrt(10 ** ((-pathloss_irs_bs) / 10))\n",
        "\n",
        "        # tmp:(num_antenna_bs,num_user) channel between BS and user\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_user]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_user])\n",
        "        tmp = tmp * pathloss_bs_user.reshape(1, num_user)\n",
        "\n",
        "        channel_bs_user.append(tmp)\n",
        "\n",
        "        # tmp: (num_antenna_bs,num_elements_irs) channel between IRS and BS\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_elements_irs]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_antenna_bs, num_elements_irs])\n",
        "        a_bs = np.exp(1j * np.pi * aoa_bs * np.arange(num_antenna_bs))\n",
        "        a_bs = np.reshape(a_bs, [num_antenna_bs, 1])\n",
        "\n",
        "        i1 = np.arange(num_elements_irs)\n",
        "        i2 = np.floor(np.arange(num_elements_irs)/irs_Nh)\n",
        "        #a_irs_bs = np.exp(1j * np.pi * (i1*aod_irs_y+i2*aod_irs_z))\n",
        "        a_irs_bs = np.exp(1j * np.pi * (i1*aod_irs_y))\n",
        "        a_irs_bs =  np.reshape(a_irs_bs, [num_elements_irs, 1])\n",
        "        los_irs_bs = a_bs @ np.transpose(a_irs_bs.conjugate())\n",
        "        tmp = np.sqrt(Rician_factor / (1 + Rician_factor)) * los_irs_bs + np.sqrt(1/(1 + Rician_factor)) * tmp\n",
        "        tmp = tmp * pathloss_irs_bs\n",
        "        channel_bs_irs.append(tmp)\n",
        "\n",
        "        # tmp:(num_elements_irs,num_user) channel between IRS and user\n",
        "        tmp = np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_elements_irs, num_user]) \\\n",
        "              + 1j * np.random.normal(loc=0, scale=np.sqrt(0.5), size=[num_elements_irs, num_user])\n",
        "        for k in range(num_user):\n",
        "            #a_irs_user = np.exp(1j * np.pi * (i1 * aoa_irs_y[k] + i2 * aoa_irs_z[k]))\n",
        "            a_irs_user = np.exp(1j * np.pi * (i1* aoa_irs_y[k]))\n",
        "            if num_user ==1:\n",
        "                aoa_irs_y_set.append(aoa_irs_y[k])\n",
        "                pathloss_irs_user_set.append(pathloss_irs_user)\n",
        "            else:\n",
        "                raise RuntimeError('number user greater than 1 !')\n",
        "            \n",
        "            tmp[:, k] = np.sqrt(Rician_factor/(1+Rician_factor))*a_irs_user+np.sqrt(1/(1+Rician_factor))*tmp[:, k]\n",
        "            tmp[:, k] = tmp[:, k] * pathloss_irs_user[k]\n",
        "        channel_irs_user.append(tmp)\n",
        "    channels = (np.array(channel_bs_user), np.array(channel_irs_user), np.array(channel_bs_irs))\n",
        "    return channels, np.array(set_location_user), np.array(aoa_irs_y_set), np.array(pathloss_irs_user_set)\n",
        "\n",
        "\n",
        "\n",
        "def channel_complex2real(channels):\n",
        "    channel_bs_user, channel_irs_user, channel_bs_irs = channels\n",
        "    (num_sample, num_antenna_bs, num_elements_irs) = channel_bs_irs.shape\n",
        "    num_user = channel_irs_user.shape[2]\n",
        "\n",
        "    A_T_real = np.zeros([num_sample, 2 * num_elements_irs, 2 * num_antenna_bs, num_user])\n",
        "    # Hd_real = np.zeros([num_sample, 2 * num_antenna_bs, num_user])\n",
        "    set_channel_combine_irs = np.zeros([num_sample, num_antenna_bs, num_elements_irs, num_user], dtype=complex)\n",
        "\n",
        "    for kk in range(num_user):\n",
        "        channel_irs_user_k = channel_irs_user[:, :, kk]\n",
        "        channel_combine_irs = channel_bs_irs * channel_irs_user_k.reshape(num_sample, 1, num_elements_irs)\n",
        "        set_channel_combine_irs[:, :, :, kk] = channel_combine_irs\n",
        "        A_tmp_tran = np.transpose(channel_combine_irs, (0, 2, 1))\n",
        "        A_tmp_real1 = np.concatenate([A_tmp_tran.real, A_tmp_tran.imag], axis=2)\n",
        "        A_tmp_real2 = np.concatenate([-A_tmp_tran.imag, A_tmp_tran.real], axis=2)\n",
        "        A_tmp_real = np.concatenate([A_tmp_real1, A_tmp_real2], axis=1)\n",
        "        A_T_real[:, :, :, kk] = A_tmp_real\n",
        "\n",
        "    Hd_real = np.concatenate([channel_bs_user.real, channel_bs_user.imag], axis=1)\n",
        "\n",
        "    return A_T_real, Hd_real, np.array(set_channel_combine_irs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C6wfdjgtCqnX",
        "outputId": "767bef8a-0143-4158-a9c6-eab64ded85d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Mounted at /content/drive\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/ris_localization_2D_fixedi1/RNN/closeBS_largeUEdistribution/params_new_closeBS_fullRician_RSS_3D_RIS_tau_6_snr_35\n",
            "3.9308102\n",
            "WARNING:tensorflow:From <ipython-input-3-1c8c54b2f737>:199: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "True\n",
            "epoch 0   loss_test:4.66306   best_test:3.93081\n",
            "epoch 1   loss_test:5.38646   best_test:3.93081\n",
            "epoch 2   loss_test:5.43013   best_test:3.93081\n",
            "epoch 3   loss_test:7.19782   best_test:3.93081\n",
            "epoch 4   loss_test:4.75247   best_test:3.93081\n",
            "epoch 5   loss_test:8.58432   best_test:3.93081\n",
            "epoch 6   loss_test:6.59458   best_test:3.93081\n",
            "epoch 7   loss_test:6.46244   best_test:3.93081\n",
            "epoch 8   loss_test:5.21666   best_test:3.93081\n",
            "epoch 9   loss_test:6.87246   best_test:3.93081\n",
            "epoch 10   loss_test:5.51374   best_test:3.93081\n",
            "epoch 11   loss_test:4.93633   best_test:3.93081\n",
            "epoch 12   loss_test:7.33743   best_test:3.93081\n",
            "epoch 13   loss_test:4.86507   best_test:3.93081\n",
            "epoch 14   loss_test:5.88006   best_test:3.93081\n",
            "epoch 15   loss_test:5.94591   best_test:3.93081\n",
            "epoch 16   loss_test:5.77776   best_test:3.93081\n",
            "epoch 17   loss_test:5.14971   best_test:3.93081\n",
            "epoch 18   loss_test:6.42836   best_test:3.93081\n",
            "epoch 19   loss_test:11.96986   best_test:3.93081\n",
            "epoch 20   loss_test:5.63984   best_test:3.93081\n",
            "epoch 21   loss_test:5.22681   best_test:3.93081\n",
            "epoch 22   loss_test:5.56202   best_test:3.93081\n",
            "epoch 23   loss_test:4.99778   best_test:3.93081\n",
            "epoch 24   loss_test:5.42294   best_test:3.93081\n",
            "epoch 25   loss_test:8.51779   best_test:3.93081\n",
            "epoch 26   loss_test:6.41611   best_test:3.93081\n",
            "epoch 27   loss_test:6.07609   best_test:3.93081\n",
            "epoch 28   loss_test:5.89816   best_test:3.93081\n",
            "epoch 29   loss_test:7.08203   best_test:3.93081\n",
            "epoch 30   loss_test:5.40533   best_test:3.93081\n",
            "epoch 31   loss_test:5.19243   best_test:3.93081\n",
            "epoch 32   loss_test:5.14394   best_test:3.93081\n",
            "epoch 33   loss_test:5.56525   best_test:3.93081\n",
            "epoch 34   loss_test:5.70220   best_test:3.93081\n",
            "epoch 35   loss_test:8.77899   best_test:3.93081\n",
            "epoch 36   loss_test:6.72689   best_test:3.93081\n",
            "epoch 37   loss_test:5.48223   best_test:3.93081\n",
            "epoch 38   loss_test:10.33616   best_test:3.93081\n",
            "epoch 39   loss_test:8.06326   best_test:3.93081\n",
            "epoch 40   loss_test:6.42220   best_test:3.93081\n",
            "epoch 41   loss_test:5.56582   best_test:3.93081\n",
            "epoch 42   loss_test:5.82362   best_test:3.93081\n",
            "epoch 43   loss_test:5.56552   best_test:3.93081\n",
            "epoch 44   loss_test:4.96187   best_test:3.93081\n",
            "epoch 45   loss_test:4.84031   best_test:3.93081\n",
            "epoch 46   loss_test:5.34345   best_test:3.93081\n",
            "epoch 47   loss_test:4.75780   best_test:3.93081\n",
            "epoch 48   loss_test:5.13657   best_test:3.93081\n",
            "epoch 49   loss_test:5.79322   best_test:3.93081\n",
            "epoch 50   loss_test:5.37863   best_test:3.93081\n",
            "epoch 51   loss_test:5.22664   best_test:3.93081\n",
            "epoch 52   loss_test:6.63610   best_test:3.93081\n",
            "epoch 53   loss_test:4.95493   best_test:3.93081\n",
            "epoch 54   loss_test:5.57083   best_test:3.93081\n",
            "epoch 55   loss_test:4.89948   best_test:3.93081\n",
            "epoch 56   loss_test:5.78643   best_test:3.93081\n",
            "epoch 57   loss_test:6.22836   best_test:3.93081\n",
            "epoch 58   loss_test:4.93542   best_test:3.93081\n",
            "epoch 59   loss_test:5.64359   best_test:3.93081\n",
            "epoch 60   loss_test:6.22145   best_test:3.93081\n",
            "epoch 61   loss_test:4.96056   best_test:3.93081\n",
            "epoch 62   loss_test:4.70665   best_test:3.93081\n",
            "epoch 63   loss_test:4.58071   best_test:3.93081\n",
            "epoch 64   loss_test:5.03309   best_test:3.93081\n",
            "epoch 65   loss_test:4.71113   best_test:3.93081\n",
            "epoch 66   loss_test:4.52934   best_test:3.93081\n",
            "epoch 67   loss_test:4.88451   best_test:3.93081\n",
            "epoch 68   loss_test:5.31432   best_test:3.93081\n",
            "epoch 69   loss_test:4.84313   best_test:3.93081\n",
            "epoch 70   loss_test:4.90786   best_test:3.93081\n",
            "epoch 71   loss_test:4.41433   best_test:3.93081\n",
            "epoch 72   loss_test:4.86062   best_test:3.93081\n",
            "epoch 73   loss_test:10.04026   best_test:3.93081\n",
            "epoch 74   loss_test:8.77345   best_test:3.93081\n",
            "epoch 75   loss_test:5.00935   best_test:3.93081\n",
            "epoch 76   loss_test:5.86448   best_test:3.93081\n",
            "epoch 77   loss_test:6.84560   best_test:3.93081\n",
            "epoch 78   loss_test:10.37744   best_test:3.93081\n",
            "epoch 79   loss_test:6.94034   best_test:3.93081\n",
            "epoch 80   loss_test:6.40955   best_test:3.93081\n",
            "epoch 81   loss_test:6.01125   best_test:3.93081\n",
            "epoch 82   loss_test:6.26028   best_test:3.93081\n",
            "epoch 83   loss_test:6.35301   best_test:3.93081\n",
            "epoch 84   loss_test:6.05650   best_test:3.93081\n",
            "epoch 85   loss_test:10.24504   best_test:3.93081\n",
            "epoch 86   loss_test:5.67940   best_test:3.93081\n",
            "epoch 87   loss_test:7.01016   best_test:3.93081\n",
            "epoch 88   loss_test:6.43616   best_test:3.93081\n",
            "epoch 89   loss_test:10.71580   best_test:3.93081\n",
            "epoch 90   loss_test:7.53015   best_test:3.93081\n",
            "epoch 91   loss_test:5.88065   best_test:3.93081\n",
            "epoch 92   loss_test:6.19139   best_test:3.93081\n",
            "epoch 93   loss_test:5.48092   best_test:3.93081\n",
            "epoch 94   loss_test:32.80600   best_test:3.93081\n",
            "epoch 95   loss_test:8.32205   best_test:3.93081\n",
            "epoch 96   loss_test:7.34842   best_test:3.93081\n",
            "epoch 97   loss_test:6.04776   best_test:3.93081\n",
            "epoch 98   loss_test:6.36566   best_test:3.93081\n",
            "epoch 99   loss_test:5.73230   best_test:3.93081\n",
            "epoch 100   loss_test:7.01117   best_test:3.93081\n",
            "epoch 101   loss_test:7.85186   best_test:3.93081\n",
            "epoch 102   loss_test:7.67006   best_test:3.93081\n",
            "epoch 103   loss_test:15.46368   best_test:3.93081\n",
            "epoch 104   loss_test:6.05309   best_test:3.93081\n",
            "epoch 105   loss_test:5.55424   best_test:3.93081\n",
            "epoch 106   loss_test:7.37124   best_test:3.93081\n",
            "epoch 107   loss_test:8.53372   best_test:3.93081\n",
            "epoch 108   loss_test:5.20972   best_test:3.93081\n",
            "epoch 109   loss_test:6.33654   best_test:3.93081\n",
            "epoch 110   loss_test:5.80632   best_test:3.93081\n",
            "epoch 111   loss_test:5.66358   best_test:3.93081\n",
            "epoch 112   loss_test:5.42648   best_test:3.93081\n",
            "epoch 113   loss_test:5.85757   best_test:3.93081\n",
            "epoch 114   loss_test:7.77283   best_test:3.93081\n",
            "epoch 115   loss_test:4.69193   best_test:3.93081\n",
            "epoch 116   loss_test:5.31607   best_test:3.93081\n",
            "epoch 117   loss_test:5.16217   best_test:3.93081\n",
            "epoch 118   loss_test:5.53477   best_test:3.93081\n",
            "epoch 119   loss_test:4.63188   best_test:3.93081\n",
            "epoch 120   loss_test:10.51157   best_test:3.93081\n",
            "epoch 121   loss_test:5.31631   best_test:3.93081\n",
            "epoch 122   loss_test:6.12408   best_test:3.93081\n",
            "epoch 123   loss_test:4.64858   best_test:3.93081\n",
            "epoch 124   loss_test:5.46835   best_test:3.93081\n",
            "epoch 125   loss_test:4.37230   best_test:3.93081\n",
            "epoch 126   loss_test:7.86442   best_test:3.93081\n",
            "epoch 127   loss_test:4.85174   best_test:3.93081\n",
            "epoch 128   loss_test:7.15878   best_test:3.93081\n",
            "epoch 129   loss_test:5.47061   best_test:3.93081\n",
            "epoch 130   loss_test:5.06000   best_test:3.93081\n",
            "epoch 131   loss_test:8.78420   best_test:3.93081\n",
            "epoch 132   loss_test:6.98118   best_test:3.93081\n",
            "epoch 133   loss_test:6.59032   best_test:3.93081\n",
            "epoch 134   loss_test:5.43044   best_test:3.93081\n",
            "epoch 135   loss_test:5.48290   best_test:3.93081\n",
            "epoch 136   loss_test:4.80845   best_test:3.93081\n",
            "epoch 137   loss_test:7.00417   best_test:3.93081\n",
            "epoch 138   loss_test:5.59586   best_test:3.93081\n",
            "epoch 139   loss_test:5.19424   best_test:3.93081\n",
            "epoch 140   loss_test:4.92054   best_test:3.93081\n",
            "epoch 141   loss_test:6.34609   best_test:3.93081\n",
            "epoch 142   loss_test:5.39031   best_test:3.93081\n",
            "epoch 143   loss_test:11.13770   best_test:3.93081\n",
            "epoch 144   loss_test:6.84555   best_test:3.93081\n",
            "epoch 145   loss_test:6.21324   best_test:3.93081\n",
            "epoch 146   loss_test:6.92682   best_test:3.93081\n",
            "epoch 147   loss_test:6.65455   best_test:3.93081\n",
            "epoch 148   loss_test:5.48044   best_test:3.93081\n",
            "epoch 149   loss_test:6.63720   best_test:3.93081\n",
            "epoch 150   loss_test:5.05903   best_test:3.93081\n",
            "epoch 151   loss_test:5.79972   best_test:3.93081\n",
            "epoch 152   loss_test:4.97764   best_test:3.93081\n",
            "epoch 153   loss_test:7.62841   best_test:3.93081\n",
            "epoch 154   loss_test:5.76274   best_test:3.93081\n",
            "epoch 155   loss_test:22.90850   best_test:3.93081\n",
            "epoch 156   loss_test:5.17234   best_test:3.93081\n",
            "epoch 157   loss_test:6.20506   best_test:3.93081\n",
            "epoch 158   loss_test:32.48565   best_test:3.93081\n",
            "epoch 159   loss_test:13.04213   best_test:3.93081\n",
            "epoch 160   loss_test:8.89355   best_test:3.93081\n",
            "epoch 161   loss_test:7.55023   best_test:3.93081\n",
            "epoch 162   loss_test:7.34309   best_test:3.93081\n",
            "epoch 163   loss_test:6.73690   best_test:3.93081\n",
            "epoch 164   loss_test:6.71395   best_test:3.93081\n",
            "epoch 165   loss_test:6.42031   best_test:3.93081\n",
            "epoch 166   loss_test:5.74104   best_test:3.93081\n",
            "epoch 167   loss_test:6.27239   best_test:3.93081\n",
            "epoch 168   loss_test:6.27500   best_test:3.93081\n",
            "epoch 169   loss_test:6.42933   best_test:3.93081\n",
            "epoch 170   loss_test:5.64713   best_test:3.93081\n",
            "epoch 171   loss_test:6.14215   best_test:3.93081\n",
            "epoch 172   loss_test:11.27589   best_test:3.93081\n",
            "epoch 173   loss_test:7.25176   best_test:3.93081\n",
            "epoch 174   loss_test:5.36343   best_test:3.93081\n",
            "epoch 175   loss_test:6.58128   best_test:3.93081\n",
            "epoch 176   loss_test:6.81290   best_test:3.93081\n",
            "epoch 177   loss_test:5.09115   best_test:3.93081\n",
            "epoch 178   loss_test:5.22869   best_test:3.93081\n",
            "epoch 179   loss_test:4.93445   best_test:3.93081\n",
            "epoch 180   loss_test:6.77573   best_test:3.93081\n",
            "epoch 181   loss_test:5.21973   best_test:3.93081\n",
            "epoch 182   loss_test:5.90252   best_test:3.93081\n",
            "epoch 183   loss_test:6.37770   best_test:3.93081\n",
            "epoch 184   loss_test:5.99129   best_test:3.93081\n",
            "epoch 185   loss_test:6.44602   best_test:3.93081\n",
            "epoch 186   loss_test:7.10875   best_test:3.93081\n",
            "epoch 187   loss_test:5.70013   best_test:3.93081\n",
            "epoch 188   loss_test:6.86053   best_test:3.93081\n",
            "epoch 189   loss_test:4.53597   best_test:3.93081\n",
            "epoch 190   loss_test:10.29351   best_test:3.93081\n",
            "epoch 191   loss_test:5.65941   best_test:3.93081\n",
            "epoch 192   loss_test:5.55824   best_test:3.93081\n",
            "epoch 193   loss_test:8.91250   best_test:3.93081\n",
            "epoch 194   loss_test:6.98990   best_test:3.93081\n",
            "epoch 195   loss_test:5.30639   best_test:3.93081\n",
            "epoch 196   loss_test:5.19867   best_test:3.93081\n",
            "epoch 197   loss_test:4.79376   best_test:3.93081\n",
            "epoch 198   loss_test:5.06773   best_test:3.93081\n",
            "epoch 199   loss_test:4.73756   best_test:3.93081\n",
            "epoch 200   loss_test:4.82022   best_test:3.93081\n",
            "epoch 201   loss_test:7.60471   best_test:3.93081\n",
            "epoch 202   loss_test:5.33136   best_test:3.93081\n",
            "epoch 203   loss_test:6.42225   best_test:3.93081\n",
            "epoch 204   loss_test:6.72873   best_test:3.93081\n",
            "epoch 205   loss_test:7.13476   best_test:3.93081\n",
            "epoch 206   loss_test:5.56017   best_test:3.93081\n",
            "epoch 207   loss_test:5.53468   best_test:3.93081\n",
            "epoch 208   loss_test:7.74368   best_test:3.93081\n",
            "epoch 209   loss_test:6.98447   best_test:3.93081\n",
            "epoch 210   loss_test:7.61983   best_test:3.93081\n",
            "epoch 211   loss_test:9.86086   best_test:3.93081\n",
            "epoch 212   loss_test:6.76087   best_test:3.93081\n",
            "epoch 213   loss_test:11.98516   best_test:3.93081\n",
            "epoch 214   loss_test:8.00007   best_test:3.93081\n",
            "epoch 215   loss_test:8.40409   best_test:3.93081\n",
            "epoch 216   loss_test:9.05876   best_test:3.93081\n",
            "epoch 217   loss_test:6.96370   best_test:3.93081\n",
            "epoch 218   loss_test:5.66231   best_test:3.93081\n",
            "epoch 219   loss_test:5.64834   best_test:3.93081\n",
            "epoch 220   loss_test:6.32079   best_test:3.93081\n",
            "epoch 221   loss_test:6.61424   best_test:3.93081\n",
            "epoch 222   loss_test:7.09917   best_test:3.93081\n",
            "epoch 223   loss_test:6.01261   best_test:3.93081\n",
            "epoch 224   loss_test:24.56029   best_test:3.93081\n",
            "epoch 225   loss_test:14.22461   best_test:3.93081\n",
            "epoch 226   loss_test:12.11320   best_test:3.93081\n",
            "epoch 227   loss_test:9.63252   best_test:3.93081\n",
            "epoch 228   loss_test:8.83006   best_test:3.93081\n",
            "epoch 229   loss_test:8.54683   best_test:3.93081\n",
            "epoch 230   loss_test:9.08352   best_test:3.93081\n",
            "epoch 231   loss_test:8.48451   best_test:3.93081\n",
            "epoch 232   loss_test:7.97760   best_test:3.93081\n",
            "epoch 233   loss_test:8.05886   best_test:3.93081\n",
            "epoch 234   loss_test:7.16524   best_test:3.93081\n",
            "epoch 235   loss_test:7.18162   best_test:3.93081\n",
            "epoch 236   loss_test:6.74853   best_test:3.93081\n",
            "epoch 237   loss_test:7.32304   best_test:3.93081\n",
            "epoch 238   loss_test:7.17033   best_test:3.93081\n",
            "epoch 239   loss_test:6.35987   best_test:3.93081\n",
            "epoch 240   loss_test:7.64246   best_test:3.93081\n",
            "epoch 241   loss_test:8.09513   best_test:3.93081\n",
            "epoch 242   loss_test:7.13572   best_test:3.93081\n",
            "epoch 243   loss_test:7.15210   best_test:3.93081\n",
            "epoch 244   loss_test:7.59364   best_test:3.93081\n",
            "epoch 245   loss_test:7.15425   best_test:3.93081\n",
            "epoch 246   loss_test:7.59038   best_test:3.93081\n",
            "epoch 247   loss_test:6.66039   best_test:3.93081\n",
            "epoch 248   loss_test:10.29320   best_test:3.93081\n",
            "epoch 249   loss_test:8.16250   best_test:3.93081\n",
            "epoch 250   loss_test:17.30258   best_test:3.93081\n",
            "epoch 251   loss_test:7.63110   best_test:3.93081\n",
            "epoch 252   loss_test:7.32828   best_test:3.93081\n",
            "epoch 253   loss_test:6.57730   best_test:3.93081\n",
            "epoch 254   loss_test:9.62702   best_test:3.93081\n",
            "epoch 255   loss_test:8.04027   best_test:3.93081\n",
            "epoch 256   loss_test:6.96179   best_test:3.93081\n",
            "epoch 257   loss_test:6.88379   best_test:3.93081\n",
            "epoch 258   loss_test:6.43213   best_test:3.93081\n",
            "epoch 259   loss_test:7.37929   best_test:3.93081\n",
            "epoch 260   loss_test:6.51270   best_test:3.93081\n",
            "epoch 261   loss_test:6.35557   best_test:3.93081\n",
            "epoch 262   loss_test:8.26181   best_test:3.93081\n",
            "epoch 263   loss_test:7.46404   best_test:3.93081\n",
            "epoch 264   loss_test:7.91400   best_test:3.93081\n",
            "epoch 265   loss_test:6.19071   best_test:3.93081\n",
            "epoch 266   loss_test:7.15205   best_test:3.93081\n",
            "epoch 267   loss_test:6.03622   best_test:3.93081\n",
            "epoch 268   loss_test:6.00439   best_test:3.93081\n",
            "epoch 269   loss_test:5.63471   best_test:3.93081\n",
            "epoch 270   loss_test:6.19378   best_test:3.93081\n",
            "epoch 271   loss_test:5.71485   best_test:3.93081\n",
            "epoch 272   loss_test:5.92093   best_test:3.93081\n",
            "epoch 273   loss_test:5.72315   best_test:3.93081\n",
            "epoch 274   loss_test:6.90483   best_test:3.93081\n",
            "epoch 275   loss_test:7.21931   best_test:3.93081\n",
            "epoch 276   loss_test:8.80340   best_test:3.93081\n",
            "epoch 277   loss_test:5.93677   best_test:3.93081\n",
            "epoch 278   loss_test:5.89909   best_test:3.93081\n",
            "epoch 279   loss_test:6.53654   best_test:3.93081\n",
            "epoch 280   loss_test:6.20663   best_test:3.93081\n",
            "epoch 281   loss_test:5.79618   best_test:3.93081\n",
            "epoch 282   loss_test:12.30709   best_test:3.93081\n",
            "epoch 283   loss_test:6.81115   best_test:3.93081\n",
            "epoch 284   loss_test:6.50370   best_test:3.93081\n",
            "epoch 285   loss_test:5.52281   best_test:3.93081\n",
            "epoch 286   loss_test:6.77209   best_test:3.93081\n",
            "epoch 287   loss_test:5.85968   best_test:3.93081\n",
            "epoch 288   loss_test:7.14842   best_test:3.93081\n",
            "epoch 289   loss_test:7.35446   best_test:3.93081\n",
            "epoch 290   loss_test:5.71108   best_test:3.93081\n",
            "epoch 291   loss_test:5.15429   best_test:3.93081\n",
            "epoch 292   loss_test:9.38822   best_test:3.93081\n",
            "epoch 293   loss_test:5.23770   best_test:3.93081\n",
            "epoch 294   loss_test:7.09870   best_test:3.93081\n",
            "epoch 295   loss_test:6.00803   best_test:3.93081\n",
            "epoch 296   loss_test:5.79418   best_test:3.93081\n",
            "epoch 297   loss_test:5.87195   best_test:3.93081\n",
            "epoch 298   loss_test:5.68567   best_test:3.93081\n",
            "epoch 299   loss_test:7.65593   best_test:3.93081\n",
            "epoch 300   loss_test:5.42970   best_test:3.93081\n",
            "epoch 301   loss_test:6.17844   best_test:3.93081\n",
            "epoch 302   loss_test:5.57719   best_test:3.93081\n",
            "epoch 303   loss_test:5.37998   best_test:3.93081\n",
            "epoch 304   loss_test:9.13043   best_test:3.93081\n",
            "epoch 305   loss_test:5.86287   best_test:3.93081\n",
            "epoch 306   loss_test:6.67401   best_test:3.93081\n",
            "epoch 307   loss_test:4.99568   best_test:3.93081\n",
            "epoch 308   loss_test:6.11129   best_test:3.93081\n",
            "epoch 309   loss_test:10.47559   best_test:3.93081\n",
            "epoch 310   loss_test:8.81161   best_test:3.93081\n",
            "epoch 311   loss_test:5.76697   best_test:3.93081\n",
            "epoch 312   loss_test:6.36226   best_test:3.93081\n",
            "epoch 313   loss_test:7.62055   best_test:3.93081\n",
            "epoch 314   loss_test:8.19239   best_test:3.93081\n",
            "epoch 315   loss_test:5.74519   best_test:3.93081\n",
            "epoch 316   loss_test:14.85117   best_test:3.93081\n",
            "epoch 317   loss_test:6.89913   best_test:3.93081\n",
            "epoch 318   loss_test:6.27050   best_test:3.93081\n",
            "epoch 319   loss_test:7.20684   best_test:3.93081\n",
            "epoch 320   loss_test:5.31435   best_test:3.93081\n",
            "epoch 321   loss_test:5.06025   best_test:3.93081\n",
            "epoch 322   loss_test:6.66993   best_test:3.93081\n",
            "epoch 323   loss_test:5.69777   best_test:3.93081\n",
            "epoch 324   loss_test:6.26057   best_test:3.93081\n",
            "epoch 325   loss_test:10.51868   best_test:3.93081\n",
            "epoch 326   loss_test:7.34851   best_test:3.93081\n",
            "epoch 327   loss_test:5.59684   best_test:3.93081\n",
            "epoch 328   loss_test:5.59015   best_test:3.93081\n",
            "epoch 329   loss_test:6.93653   best_test:3.93081\n",
            "epoch 330   loss_test:5.66622   best_test:3.93081\n",
            "epoch 331   loss_test:8.11318   best_test:3.93081\n",
            "epoch 332   loss_test:5.44363   best_test:3.93081\n",
            "epoch 333   loss_test:6.71199   best_test:3.93081\n",
            "epoch 334   loss_test:6.33781   best_test:3.93081\n",
            "epoch 335   loss_test:5.77241   best_test:3.93081\n",
            "epoch 336   loss_test:5.44831   best_test:3.93081\n",
            "epoch 337   loss_test:5.82947   best_test:3.93081\n",
            "epoch 338   loss_test:6.73487   best_test:3.93081\n",
            "epoch 339   loss_test:6.47530   best_test:3.93081\n",
            "epoch 340   loss_test:7.06934   best_test:3.93081\n",
            "epoch 341   loss_test:6.76359   best_test:3.93081\n",
            "epoch 342   loss_test:6.49095   best_test:3.93081\n",
            "epoch 343   loss_test:5.62214   best_test:3.93081\n",
            "epoch 344   loss_test:5.24739   best_test:3.93081\n",
            "epoch 345   loss_test:7.20661   best_test:3.93081\n",
            "epoch 346   loss_test:5.52164   best_test:3.93081\n",
            "epoch 347   loss_test:5.80258   best_test:3.93081\n",
            "epoch 348   loss_test:5.89361   best_test:3.93081\n",
            "epoch 349   loss_test:5.04571   best_test:3.93081\n",
            "epoch 350   loss_test:5.91877   best_test:3.93081\n",
            "epoch 351   loss_test:6.19553   best_test:3.93081\n",
            "epoch 352   loss_test:5.53125   best_test:3.93081\n",
            "epoch 353   loss_test:5.70775   best_test:3.93081\n",
            "epoch 354   loss_test:5.65450   best_test:3.93081\n",
            "epoch 355   loss_test:12.39879   best_test:3.93081\n",
            "epoch 356   loss_test:6.57502   best_test:3.93081\n",
            "epoch 357   loss_test:13.23685   best_test:3.93081\n",
            "epoch 358   loss_test:6.68618   best_test:3.93081\n",
            "epoch 359   loss_test:6.12230   best_test:3.93081\n",
            "epoch 360   loss_test:6.64996   best_test:3.93081\n",
            "epoch 361   loss_test:20.29716   best_test:3.93081\n",
            "epoch 362   loss_test:8.20172   best_test:3.93081\n",
            "epoch 363   loss_test:6.97463   best_test:3.93081\n",
            "epoch 364   loss_test:7.07567   best_test:3.93081\n",
            "epoch 365   loss_test:6.84703   best_test:3.93081\n",
            "epoch 366   loss_test:6.45199   best_test:3.93081\n",
            "epoch 367   loss_test:6.13665   best_test:3.93081\n",
            "epoch 368   loss_test:5.90320   best_test:3.93081\n",
            "epoch 369   loss_test:5.99521   best_test:3.93081\n",
            "epoch 370   loss_test:8.14691   best_test:3.93081\n",
            "epoch 371   loss_test:5.86571   best_test:3.93081\n",
            "epoch 372   loss_test:7.12859   best_test:3.93081\n",
            "epoch 373   loss_test:6.00724   best_test:3.93081\n",
            "epoch 374   loss_test:5.70627   best_test:3.93081\n",
            "epoch 375   loss_test:6.89100   best_test:3.93081\n",
            "epoch 376   loss_test:5.39365   best_test:3.93081\n",
            "epoch 377   loss_test:7.64967   best_test:3.93081\n",
            "epoch 378   loss_test:5.74527   best_test:3.93081\n",
            "epoch 379   loss_test:23.55672   best_test:3.93081\n",
            "epoch 380   loss_test:15.93559   best_test:3.93081\n",
            "epoch 381   loss_test:30.57416   best_test:3.93081\n",
            "epoch 382   loss_test:11.48628   best_test:3.93081\n",
            "epoch 383   loss_test:10.26082   best_test:3.93081\n",
            "epoch 384   loss_test:9.49686   best_test:3.93081\n",
            "epoch 385   loss_test:12.11093   best_test:3.93081\n",
            "epoch 386   loss_test:9.24625   best_test:3.93081\n",
            "epoch 387   loss_test:8.56563   best_test:3.93081\n",
            "epoch 388   loss_test:8.86985   best_test:3.93081\n",
            "epoch 389   loss_test:8.50951   best_test:3.93081\n",
            "epoch 390   loss_test:7.84488   best_test:3.93081\n",
            "epoch 391   loss_test:7.62322   best_test:3.93081\n",
            "epoch 392   loss_test:7.61537   best_test:3.93081\n",
            "epoch 393   loss_test:7.67037   best_test:3.93081\n",
            "epoch 394   loss_test:7.61928   best_test:3.93081\n",
            "epoch 395   loss_test:7.39039   best_test:3.93081\n",
            "epoch 396   loss_test:7.00320   best_test:3.93081\n",
            "epoch 397   loss_test:7.34261   best_test:3.93081\n",
            "epoch 398   loss_test:6.55965   best_test:3.93081\n",
            "epoch 399   loss_test:6.21838   best_test:3.93081\n",
            "epoch 400   loss_test:32.15221   best_test:3.93081\n",
            "epoch 401   loss_test:9.25939   best_test:3.93081\n",
            "epoch 402   loss_test:8.49684   best_test:3.93081\n",
            "epoch 403   loss_test:6.62291   best_test:3.93081\n",
            "epoch 404   loss_test:6.48316   best_test:3.93081\n",
            "epoch 405   loss_test:6.06238   best_test:3.93081\n",
            "epoch 406   loss_test:7.11650   best_test:3.93081\n",
            "epoch 407   loss_test:6.37925   best_test:3.93081\n",
            "epoch 408   loss_test:6.30023   best_test:3.93081\n",
            "epoch 409   loss_test:8.79275   best_test:3.93081\n",
            "epoch 410   loss_test:7.12705   best_test:3.93081\n",
            "epoch 411   loss_test:7.03496   best_test:3.93081\n",
            "epoch 412   loss_test:6.34078   best_test:3.93081\n",
            "epoch 413   loss_test:6.14796   best_test:3.93081\n",
            "epoch 414   loss_test:8.96504   best_test:3.93081\n",
            "epoch 415   loss_test:7.25850   best_test:3.93081\n",
            "epoch 416   loss_test:7.63444   best_test:3.93081\n",
            "epoch 417   loss_test:6.95098   best_test:3.93081\n",
            "epoch 418   loss_test:6.10972   best_test:3.93081\n",
            "epoch 419   loss_test:7.64237   best_test:3.93081\n",
            "epoch 420   loss_test:10.78595   best_test:3.93081\n",
            "epoch 421   loss_test:5.96831   best_test:3.93081\n",
            "epoch 422   loss_test:6.68901   best_test:3.93081\n",
            "epoch 423   loss_test:6.86866   best_test:3.93081\n",
            "epoch 424   loss_test:7.04028   best_test:3.93081\n",
            "epoch 425   loss_test:7.12153   best_test:3.93081\n",
            "epoch 426   loss_test:6.38619   best_test:3.93081\n",
            "epoch 427   loss_test:6.29586   best_test:3.93081\n",
            "epoch 428   loss_test:6.80634   best_test:3.93081\n",
            "epoch 429   loss_test:6.83304   best_test:3.93081\n",
            "epoch 430   loss_test:6.34259   best_test:3.93081\n",
            "epoch 431   loss_test:6.04599   best_test:3.93081\n",
            "epoch 432   loss_test:5.95248   best_test:3.93081\n",
            "epoch 433   loss_test:7.18263   best_test:3.93081\n",
            "epoch 434   loss_test:5.35225   best_test:3.93081\n",
            "epoch 435   loss_test:5.50181   best_test:3.93081\n",
            "epoch 436   loss_test:5.86224   best_test:3.93081\n",
            "epoch 437   loss_test:5.93064   best_test:3.93081\n",
            "epoch 438   loss_test:6.63836   best_test:3.93081\n",
            "epoch 439   loss_test:6.59681   best_test:3.93081\n",
            "epoch 440   loss_test:6.83380   best_test:3.93081\n",
            "epoch 441   loss_test:5.41531   best_test:3.93081\n",
            "epoch 442   loss_test:5.87180   best_test:3.93081\n",
            "epoch 443   loss_test:6.40886   best_test:3.93081\n",
            "epoch 444   loss_test:6.24643   best_test:3.93081\n",
            "epoch 445   loss_test:5.92165   best_test:3.93081\n",
            "epoch 446   loss_test:6.88267   best_test:3.93081\n",
            "epoch 447   loss_test:10.26329   best_test:3.93081\n",
            "epoch 448   loss_test:5.58528   best_test:3.93081\n",
            "epoch 449   loss_test:5.51176   best_test:3.93081\n",
            "epoch 450   loss_test:4.82158   best_test:3.93081\n",
            "epoch 451   loss_test:5.81919   best_test:3.93081\n",
            "epoch 452   loss_test:7.54384   best_test:3.93081\n",
            "epoch 453   loss_test:5.67982   best_test:3.93081\n",
            "epoch 454   loss_test:7.95791   best_test:3.93081\n",
            "epoch 455   loss_test:11.54356   best_test:3.93081\n",
            "epoch 456   loss_test:5.75807   best_test:3.93081\n",
            "epoch 457   loss_test:10.01892   best_test:3.93081\n",
            "epoch 458   loss_test:6.37934   best_test:3.93081\n",
            "epoch 459   loss_test:7.41524   best_test:3.93081\n",
            "epoch 460   loss_test:6.27247   best_test:3.93081\n",
            "epoch 461   loss_test:7.24148   best_test:3.93081\n",
            "epoch 462   loss_test:5.79311   best_test:3.93081\n",
            "epoch 463   loss_test:6.79336   best_test:3.93081\n",
            "epoch 464   loss_test:6.95349   best_test:3.93081\n",
            "epoch 465   loss_test:9.44090   best_test:3.93081\n",
            "epoch 466   loss_test:6.28624   best_test:3.93081\n",
            "epoch 467   loss_test:6.10613   best_test:3.93081\n",
            "epoch 468   loss_test:5.87779   best_test:3.93081\n",
            "epoch 469   loss_test:5.78128   best_test:3.93081\n",
            "epoch 470   loss_test:5.59789   best_test:3.93081\n",
            "epoch 471   loss_test:7.11126   best_test:3.93081\n",
            "epoch 472   loss_test:6.08776   best_test:3.93081\n",
            "epoch 473   loss_test:6.26282   best_test:3.93081\n",
            "epoch 474   loss_test:5.63229   best_test:3.93081\n",
            "epoch 475   loss_test:5.65959   best_test:3.93081\n",
            "epoch 476   loss_test:7.28606   best_test:3.93081\n",
            "epoch 477   loss_test:5.50414   best_test:3.93081\n",
            "epoch 478   loss_test:5.68311   best_test:3.93081\n",
            "epoch 479   loss_test:6.28846   best_test:3.93081\n",
            "epoch 480   loss_test:5.41029   best_test:3.93081\n",
            "epoch 481   loss_test:6.45093   best_test:3.93081\n",
            "epoch 482   loss_test:5.36389   best_test:3.93081\n",
            "epoch 483   loss_test:4.88054   best_test:3.93081\n",
            "epoch 484   loss_test:7.06765   best_test:3.93081\n",
            "epoch 485   loss_test:5.75140   best_test:3.93081\n",
            "epoch 486   loss_test:6.34613   best_test:3.93081\n",
            "epoch 487   loss_test:5.49750   best_test:3.93081\n",
            "epoch 488   loss_test:5.62284   best_test:3.93081\n",
            "epoch 489   loss_test:7.38516   best_test:3.93081\n",
            "epoch 490   loss_test:9.60324   best_test:3.93081\n",
            "epoch 491   loss_test:5.73011   best_test:3.93081\n",
            "epoch 492   loss_test:5.30246   best_test:3.93081\n",
            "epoch 493   loss_test:6.29653   best_test:3.93081\n",
            "epoch 494   loss_test:5.40586   best_test:3.93081\n",
            "epoch 495   loss_test:5.44985   best_test:3.93081\n",
            "epoch 496   loss_test:6.24064   best_test:3.93081\n",
            "epoch 497   loss_test:17.67293   best_test:3.93081\n",
            "epoch 498   loss_test:6.68447   best_test:3.93081\n",
            "epoch 499   loss_test:5.79983   best_test:3.93081\n",
            "epoch 500   loss_test:5.61170   best_test:3.93081\n",
            "epoch 501   loss_test:7.00899   best_test:3.93081\n",
            "epoch 502   loss_test:5.76814   best_test:3.93081\n",
            "epoch 503   loss_test:5.68127   best_test:3.93081\n",
            "epoch 504   loss_test:5.90667   best_test:3.93081\n",
            "epoch 505   loss_test:5.14056   best_test:3.93081\n",
            "epoch 506   loss_test:9.37332   best_test:3.93081\n",
            "epoch 507   loss_test:6.73906   best_test:3.93081\n",
            "epoch 508   loss_test:6.26376   best_test:3.93081\n",
            "epoch 509   loss_test:6.81238   best_test:3.93081\n",
            "epoch 510   loss_test:10.02814   best_test:3.93081\n",
            "epoch 511   loss_test:9.84484   best_test:3.93081\n",
            "epoch 512   loss_test:5.74762   best_test:3.93081\n",
            "epoch 513   loss_test:5.32329   best_test:3.93081\n",
            "epoch 514   loss_test:6.28082   best_test:3.93081\n",
            "epoch 515   loss_test:6.54533   best_test:3.93081\n",
            "epoch 516   loss_test:6.18894   best_test:3.93081\n",
            "epoch 517   loss_test:6.57264   best_test:3.93081\n",
            "epoch 518   loss_test:5.55758   best_test:3.93081\n",
            "epoch 519   loss_test:8.95655   best_test:3.93081\n",
            "epoch 520   loss_test:5.74034   best_test:3.93081\n",
            "epoch 521   loss_test:8.51177   best_test:3.93081\n",
            "epoch 522   loss_test:6.40894   best_test:3.93081\n",
            "epoch 523   loss_test:7.02883   best_test:3.93081\n",
            "epoch 524   loss_test:7.76484   best_test:3.93081\n",
            "epoch 525   loss_test:6.54456   best_test:3.93081\n",
            "epoch 526   loss_test:15.72724   best_test:3.93081\n",
            "epoch 527   loss_test:7.19711   best_test:3.93081\n",
            "epoch 528   loss_test:7.48694   best_test:3.93081\n",
            "epoch 529   loss_test:6.05626   best_test:3.93081\n",
            "epoch 530   loss_test:5.50925   best_test:3.93081\n",
            "epoch 531   loss_test:5.71900   best_test:3.93081\n",
            "epoch 532   loss_test:6.07941   best_test:3.93081\n",
            "epoch 533   loss_test:5.32251   best_test:3.93081\n",
            "epoch 534   loss_test:9.07321   best_test:3.93081\n",
            "epoch 535   loss_test:5.06725   best_test:3.93081\n",
            "epoch 536   loss_test:6.34340   best_test:3.93081\n",
            "epoch 537   loss_test:8.92648   best_test:3.93081\n",
            "epoch 538   loss_test:8.38144   best_test:3.93081\n",
            "epoch 539   loss_test:5.81705   best_test:3.93081\n",
            "epoch 540   loss_test:6.61430   best_test:3.93081\n",
            "epoch 541   loss_test:5.64672   best_test:3.93081\n",
            "epoch 542   loss_test:5.54753   best_test:3.93081\n",
            "epoch 543   loss_test:5.81722   best_test:3.93081\n",
            "epoch 544   loss_test:5.52109   best_test:3.93081\n",
            "epoch 545   loss_test:6.65290   best_test:3.93081\n",
            "epoch 546   loss_test:8.76314   best_test:3.93081\n",
            "epoch 547   loss_test:5.55702   best_test:3.93081\n",
            "epoch 548   loss_test:5.96700   best_test:3.93081\n",
            "epoch 549   loss_test:6.78264   best_test:3.93081\n",
            "epoch 550   loss_test:6.50946   best_test:3.93081\n",
            "epoch 551   loss_test:7.04567   best_test:3.93081\n",
            "epoch 552   loss_test:6.13305   best_test:3.93081\n",
            "epoch 553   loss_test:7.14499   best_test:3.93081\n",
            "epoch 554   loss_test:5.75519   best_test:3.93081\n",
            "epoch 555   loss_test:6.16231   best_test:3.93081\n",
            "epoch 556   loss_test:6.01625   best_test:3.93081\n",
            "epoch 557   loss_test:8.58043   best_test:3.93081\n",
            "epoch 558   loss_test:6.09642   best_test:3.93081\n",
            "epoch 559   loss_test:5.68517   best_test:3.93081\n",
            "epoch 560   loss_test:6.40156   best_test:3.93081\n",
            "epoch 561   loss_test:7.08623   best_test:3.93081\n",
            "epoch 562   loss_test:9.32132   best_test:3.93081\n",
            "epoch 563   loss_test:13.86112   best_test:3.93081\n",
            "epoch 564   loss_test:7.58683   best_test:3.93081\n",
            "epoch 565   loss_test:7.19393   best_test:3.93081\n",
            "epoch 566   loss_test:6.83857   best_test:3.93081\n",
            "epoch 567   loss_test:6.65345   best_test:3.93081\n",
            "epoch 568   loss_test:6.03538   best_test:3.93081\n",
            "epoch 569   loss_test:6.87817   best_test:3.93081\n",
            "epoch 570   loss_test:7.92667   best_test:3.93081\n",
            "epoch 571   loss_test:6.66458   best_test:3.93081\n",
            "epoch 572   loss_test:6.87963   best_test:3.93081\n",
            "epoch 573   loss_test:6.65650   best_test:3.93081\n",
            "epoch 574   loss_test:5.96173   best_test:3.93081\n",
            "epoch 575   loss_test:5.66492   best_test:3.93081\n",
            "epoch 576   loss_test:6.98171   best_test:3.93081\n",
            "epoch 577   loss_test:5.21568   best_test:3.93081\n",
            "epoch 578   loss_test:6.30972   best_test:3.93081\n",
            "epoch 579   loss_test:7.46918   best_test:3.93081\n",
            "epoch 580   loss_test:6.23235   best_test:3.93081\n",
            "epoch 581   loss_test:6.01099   best_test:3.93081\n",
            "epoch 582   loss_test:7.36613   best_test:3.93081\n",
            "epoch 583   loss_test:5.25234   best_test:3.93081\n",
            "epoch 584   loss_test:5.07380   best_test:3.93081\n",
            "epoch 585   loss_test:4.91147   best_test:3.93081\n",
            "epoch 586   loss_test:5.76752   best_test:3.93081\n",
            "epoch 587   loss_test:6.09449   best_test:3.93081\n",
            "epoch 588   loss_test:6.79280   best_test:3.93081\n",
            "epoch 589   loss_test:6.04793   best_test:3.93081\n",
            "epoch 590   loss_test:5.55750   best_test:3.93081\n",
            "epoch 591   loss_test:5.26352   best_test:3.93081\n",
            "epoch 592   loss_test:6.46456   best_test:3.93081\n",
            "epoch 593   loss_test:7.02171   best_test:3.93081\n",
            "epoch 594   loss_test:8.25059   best_test:3.93081\n",
            "epoch 595   loss_test:6.20710   best_test:3.93081\n",
            "epoch 596   loss_test:6.95072   best_test:3.93081\n",
            "epoch 597   loss_test:5.88653   best_test:3.93081\n",
            "epoch 598   loss_test:6.43602   best_test:3.93081\n",
            "epoch 599   loss_test:6.70960   best_test:3.93081\n",
            "epoch 600   loss_test:11.31048   best_test:3.93081\n",
            "epoch 601   loss_test:5.76121   best_test:3.93081\n",
            "epoch 602   loss_test:5.86885   best_test:3.93081\n",
            "epoch 603   loss_test:5.79836   best_test:3.93081\n",
            "epoch 604   loss_test:7.38359   best_test:3.93081\n",
            "epoch 605   loss_test:5.46846   best_test:3.93081\n",
            "epoch 606   loss_test:6.15013   best_test:3.93081\n",
            "epoch 607   loss_test:5.78835   best_test:3.93081\n",
            "epoch 608   loss_test:5.74267   best_test:3.93081\n",
            "epoch 609   loss_test:4.96950   best_test:3.93081\n",
            "epoch 610   loss_test:5.43079   best_test:3.93081\n",
            "epoch 611   loss_test:7.04810   best_test:3.93081\n",
            "epoch 612   loss_test:6.52293   best_test:3.93081\n",
            "epoch 613   loss_test:7.36489   best_test:3.93081\n",
            "epoch 614   loss_test:6.04115   best_test:3.93081\n",
            "epoch 615   loss_test:5.01411   best_test:3.93081\n",
            "epoch 616   loss_test:6.46696   best_test:3.93081\n",
            "epoch 617   loss_test:8.51391   best_test:3.93081\n",
            "epoch 618   loss_test:6.34232   best_test:3.93081\n",
            "epoch 619   loss_test:7.82831   best_test:3.93081\n",
            "epoch 620   loss_test:6.47611   best_test:3.93081\n",
            "epoch 621   loss_test:7.77150   best_test:3.93081\n",
            "epoch 622   loss_test:5.58575   best_test:3.93081\n",
            "epoch 623   loss_test:6.28417   best_test:3.93081\n",
            "epoch 624   loss_test:5.65127   best_test:3.93081\n",
            "epoch 625   loss_test:4.88759   best_test:3.93081\n",
            "epoch 626   loss_test:5.60180   best_test:3.93081\n",
            "epoch 627   loss_test:6.22954   best_test:3.93081\n",
            "epoch 628   loss_test:5.11684   best_test:3.93081\n",
            "epoch 629   loss_test:6.35512   best_test:3.93081\n",
            "epoch 630   loss_test:5.81746   best_test:3.93081\n",
            "epoch 631   loss_test:5.08559   best_test:3.93081\n",
            "epoch 632   loss_test:16.10247   best_test:3.93081\n",
            "epoch 633   loss_test:6.48879   best_test:3.93081\n",
            "epoch 634   loss_test:5.55157   best_test:3.93081\n",
            "epoch 635   loss_test:5.99084   best_test:3.93081\n",
            "epoch 636   loss_test:7.93574   best_test:3.93081\n",
            "epoch 637   loss_test:5.79060   best_test:3.93081\n",
            "epoch 638   loss_test:9.97980   best_test:3.93081\n",
            "epoch 639   loss_test:7.27371   best_test:3.93081\n",
            "epoch 640   loss_test:5.49709   best_test:3.93081\n",
            "epoch 641   loss_test:5.15273   best_test:3.93081\n",
            "epoch 642   loss_test:8.81945   best_test:3.93081\n",
            "epoch 643   loss_test:7.20043   best_test:3.93081\n",
            "epoch 644   loss_test:7.67322   best_test:3.93081\n",
            "epoch 645   loss_test:6.06046   best_test:3.93081\n",
            "epoch 646   loss_test:8.33327   best_test:3.93081\n",
            "epoch 647   loss_test:21.78052   best_test:3.93081\n",
            "epoch 648   loss_test:8.61469   best_test:3.93081\n",
            "epoch 649   loss_test:8.49525   best_test:3.93081\n",
            "epoch 650   loss_test:7.55638   best_test:3.93081\n",
            "epoch 651   loss_test:5.50129   best_test:3.93081\n",
            "epoch 652   loss_test:7.05258   best_test:3.93081\n",
            "epoch 653   loss_test:7.49405   best_test:3.93081\n",
            "epoch 654   loss_test:5.24750   best_test:3.93081\n",
            "epoch 655   loss_test:8.70982   best_test:3.93081\n",
            "epoch 656   loss_test:5.88449   best_test:3.93081\n",
            "epoch 657   loss_test:6.11797   best_test:3.93081\n",
            "epoch 658   loss_test:19.58798   best_test:3.93081\n",
            "epoch 659   loss_test:6.35068   best_test:3.93081\n",
            "epoch 660   loss_test:8.12304   best_test:3.93081\n",
            "epoch 661   loss_test:10.66332   best_test:3.93081\n",
            "epoch 662   loss_test:7.86506   best_test:3.93081\n",
            "epoch 663   loss_test:9.73454   best_test:3.93081\n",
            "epoch 664   loss_test:7.28581   best_test:3.93081\n",
            "epoch 665   loss_test:5.71313   best_test:3.93081\n",
            "epoch 666   loss_test:5.39225   best_test:3.93081\n",
            "epoch 667   loss_test:6.29145   best_test:3.93081\n",
            "epoch 668   loss_test:10.83243   best_test:3.93081\n",
            "epoch 669   loss_test:6.68451   best_test:3.93081\n",
            "epoch 670   loss_test:5.96231   best_test:3.93081\n",
            "epoch 671   loss_test:6.15811   best_test:3.93081\n",
            "epoch 672   loss_test:5.90314   best_test:3.93081\n",
            "epoch 673   loss_test:6.67592   best_test:3.93081\n",
            "epoch 674   loss_test:5.57734   best_test:3.93081\n",
            "epoch 675   loss_test:5.57264   best_test:3.93081\n",
            "epoch 676   loss_test:5.42546   best_test:3.93081\n",
            "epoch 677   loss_test:6.27427   best_test:3.93081\n",
            "epoch 678   loss_test:5.81178   best_test:3.93081\n",
            "epoch 679   loss_test:5.73018   best_test:3.93081\n",
            "epoch 680   loss_test:5.49678   best_test:3.93081\n",
            "epoch 681   loss_test:8.92202   best_test:3.93081\n",
            "epoch 682   loss_test:5.06825   best_test:3.93081\n",
            "epoch 683   loss_test:5.45743   best_test:3.93081\n",
            "epoch 684   loss_test:6.99517   best_test:3.93081\n",
            "epoch 685   loss_test:5.29446   best_test:3.93081\n",
            "epoch 686   loss_test:9.42577   best_test:3.93081\n",
            "epoch 687   loss_test:4.94391   best_test:3.93081\n",
            "epoch 688   loss_test:6.03898   best_test:3.93081\n",
            "epoch 689   loss_test:6.74581   best_test:3.93081\n",
            "epoch 690   loss_test:5.83225   best_test:3.93081\n",
            "epoch 691   loss_test:5.81686   best_test:3.93081\n",
            "epoch 692   loss_test:8.37890   best_test:3.93081\n",
            "epoch 693   loss_test:5.87278   best_test:3.93081\n",
            "epoch 694   loss_test:5.44669   best_test:3.93081\n",
            "epoch 695   loss_test:7.33968   best_test:3.93081\n",
            "epoch 696   loss_test:10.03646   best_test:3.93081\n",
            "epoch 697   loss_test:5.55746   best_test:3.93081\n",
            "epoch 698   loss_test:14.22968   best_test:3.93081\n",
            "epoch 699   loss_test:9.69591   best_test:3.93081\n",
            "epoch 700   loss_test:8.18230   best_test:3.93081\n",
            "epoch 701   loss_test:10.73235   best_test:3.93081\n",
            "epoch 702   loss_test:8.93587   best_test:3.93081\n",
            "epoch 703   loss_test:7.13782   best_test:3.93081\n",
            "epoch 704   loss_test:6.98224   best_test:3.93081\n",
            "epoch 705   loss_test:6.30070   best_test:3.93081\n",
            "epoch 706   loss_test:8.67451   best_test:3.93081\n",
            "epoch 707   loss_test:6.28677   best_test:3.93081\n",
            "epoch 708   loss_test:6.32457   best_test:3.93081\n",
            "epoch 709   loss_test:7.65710   best_test:3.93081\n",
            "epoch 710   loss_test:6.71261   best_test:3.93081\n",
            "epoch 711   loss_test:6.48659   best_test:3.93081\n",
            "epoch 712   loss_test:5.78173   best_test:3.93081\n",
            "epoch 713   loss_test:6.42566   best_test:3.93081\n",
            "epoch 714   loss_test:6.62797   best_test:3.93081\n",
            "epoch 715   loss_test:6.47965   best_test:3.93081\n",
            "epoch 716   loss_test:6.21710   best_test:3.93081\n",
            "epoch 717   loss_test:5.56534   best_test:3.93081\n",
            "epoch 718   loss_test:5.92185   best_test:3.93081\n",
            "epoch 719   loss_test:6.19614   best_test:3.93081\n",
            "epoch 720   loss_test:5.70608   best_test:3.93081\n",
            "epoch 721   loss_test:5.83612   best_test:3.93081\n",
            "epoch 722   loss_test:6.05364   best_test:3.93081\n",
            "epoch 723   loss_test:5.79407   best_test:3.93081\n",
            "epoch 724   loss_test:6.21363   best_test:3.93081\n",
            "epoch 725   loss_test:5.70458   best_test:3.93081\n",
            "epoch 726   loss_test:7.47626   best_test:3.93081\n",
            "epoch 727   loss_test:6.25970   best_test:3.93081\n",
            "epoch 728   loss_test:5.95978   best_test:3.93081\n",
            "epoch 729   loss_test:6.32688   best_test:3.93081\n",
            "epoch 730   loss_test:5.56638   best_test:3.93081\n",
            "epoch 731   loss_test:5.53782   best_test:3.93081\n",
            "epoch 732   loss_test:5.61002   best_test:3.93081\n",
            "epoch 733   loss_test:5.56020   best_test:3.93081\n",
            "epoch 734   loss_test:5.12891   best_test:3.93081\n",
            "epoch 735   loss_test:9.45627   best_test:3.93081\n",
            "epoch 736   loss_test:5.35695   best_test:3.93081\n",
            "epoch 737   loss_test:5.75539   best_test:3.93081\n",
            "epoch 738   loss_test:5.51763   best_test:3.93081\n",
            "epoch 739   loss_test:5.55519   best_test:3.93081\n",
            "epoch 740   loss_test:6.38216   best_test:3.93081\n",
            "epoch 741   loss_test:6.55921   best_test:3.93081\n",
            "epoch 742   loss_test:5.70533   best_test:3.93081\n",
            "epoch 743   loss_test:5.93569   best_test:3.93081\n",
            "epoch 744   loss_test:4.72522   best_test:3.93081\n",
            "epoch 745   loss_test:5.03009   best_test:3.93081\n",
            "epoch 746   loss_test:12.68808   best_test:3.93081\n",
            "epoch 747   loss_test:5.17531   best_test:3.93081\n",
            "epoch 748   loss_test:5.38897   best_test:3.93081\n",
            "epoch 749   loss_test:9.32848   best_test:3.93081\n",
            "epoch 750   loss_test:6.30051   best_test:3.93081\n",
            "epoch 751   loss_test:5.87611   best_test:3.93081\n",
            "epoch 752   loss_test:6.39407   best_test:3.93081\n",
            "epoch 753   loss_test:5.47302   best_test:3.93081\n",
            "epoch 754   loss_test:5.66065   best_test:3.93081\n",
            "epoch 755   loss_test:6.02202   best_test:3.93081\n",
            "epoch 756   loss_test:5.11406   best_test:3.93081\n",
            "epoch 757   loss_test:5.14861   best_test:3.93081\n",
            "epoch 758   loss_test:5.77851   best_test:3.93081\n",
            "epoch 759   loss_test:7.27209   best_test:3.93081\n",
            "epoch 760   loss_test:5.12421   best_test:3.93081\n",
            "epoch 761   loss_test:13.35129   best_test:3.93081\n",
            "epoch 762   loss_test:5.88730   best_test:3.93081\n",
            "epoch 763   loss_test:6.45338   best_test:3.93081\n",
            "epoch 764   loss_test:9.70390   best_test:3.93081\n",
            "epoch 765   loss_test:6.76026   best_test:3.93081\n",
            "epoch 766   loss_test:5.79090   best_test:3.93081\n",
            "epoch 767   loss_test:6.66736   best_test:3.93081\n",
            "epoch 768   loss_test:10.84074   best_test:3.93081\n",
            "epoch 769   loss_test:5.43511   best_test:3.93081\n",
            "epoch 770   loss_test:5.08861   best_test:3.93081\n",
            "epoch 771   loss_test:8.01810   best_test:3.93081\n",
            "epoch 772   loss_test:5.41299   best_test:3.93081\n",
            "epoch 773   loss_test:6.09482   best_test:3.93081\n",
            "epoch 774   loss_test:7.97741   best_test:3.93081\n",
            "epoch 775   loss_test:6.38047   best_test:3.93081\n",
            "epoch 776   loss_test:7.43222   best_test:3.93081\n",
            "epoch 777   loss_test:8.97906   best_test:3.93081\n",
            "epoch 778   loss_test:6.24055   best_test:3.93081\n",
            "epoch 779   loss_test:9.30305   best_test:3.93081\n",
            "epoch 780   loss_test:5.44939   best_test:3.93081\n",
            "epoch 781   loss_test:6.18628   best_test:3.93081\n",
            "epoch 782   loss_test:5.53485   best_test:3.93081\n",
            "epoch 783   loss_test:6.32123   best_test:3.93081\n",
            "epoch 784   loss_test:6.97278   best_test:3.93081\n",
            "epoch 785   loss_test:6.29453   best_test:3.93081\n",
            "epoch 786   loss_test:5.56636   best_test:3.93081\n",
            "epoch 787   loss_test:6.59193   best_test:3.93081\n",
            "epoch 788   loss_test:7.15683   best_test:3.93081\n",
            "epoch 789   loss_test:5.87168   best_test:3.93081\n",
            "epoch 790   loss_test:5.33312   best_test:3.93081\n",
            "epoch 791   loss_test:5.55626   best_test:3.93081\n",
            "epoch 792   loss_test:5.05059   best_test:3.93081\n",
            "epoch 793   loss_test:4.92194   best_test:3.93081\n",
            "epoch 794   loss_test:5.55861   best_test:3.93081\n",
            "epoch 795   loss_test:5.07228   best_test:3.93081\n",
            "epoch 796   loss_test:5.29085   best_test:3.93081\n",
            "epoch 797   loss_test:5.36270   best_test:3.93081\n",
            "epoch 798   loss_test:4.66336   best_test:3.93081\n",
            "epoch 799   loss_test:5.08528   best_test:3.93081\n",
            "epoch 800   loss_test:5.16978   best_test:3.93081\n",
            "epoch 801   loss_test:5.95361   best_test:3.93081\n",
            "epoch 802   loss_test:8.55495   best_test:3.93081\n",
            "epoch 803   loss_test:13.66873   best_test:3.93081\n",
            "epoch 804   loss_test:5.49459   best_test:3.93081\n",
            "epoch 805   loss_test:5.27937   best_test:3.93081\n",
            "epoch 806   loss_test:4.67896   best_test:3.93081\n",
            "epoch 807   loss_test:5.03354   best_test:3.93081\n",
            "epoch 808   loss_test:5.39616   best_test:3.93081\n",
            "epoch 809   loss_test:5.25372   best_test:3.93081\n",
            "epoch 810   loss_test:4.39104   best_test:3.93081\n",
            "epoch 811   loss_test:4.40495   best_test:3.93081\n",
            "epoch 812   loss_test:9.19184   best_test:3.93081\n",
            "epoch 813   loss_test:5.45997   best_test:3.93081\n",
            "epoch 814   loss_test:5.41967   best_test:3.93081\n",
            "epoch 815   loss_test:5.95054   best_test:3.93081\n",
            "epoch 816   loss_test:12.00908   best_test:3.93081\n",
            "epoch 817   loss_test:5.57528   best_test:3.93081\n",
            "epoch 818   loss_test:5.38603   best_test:3.93081\n",
            "epoch 819   loss_test:5.27518   best_test:3.93081\n",
            "epoch 820   loss_test:14.23758   best_test:3.93081\n",
            "epoch 821   loss_test:7.56326   best_test:3.93081\n",
            "epoch 822   loss_test:5.14777   best_test:3.93081\n",
            "epoch 823   loss_test:5.37776   best_test:3.93081\n",
            "epoch 824   loss_test:4.94561   best_test:3.93081\n",
            "epoch 825   loss_test:5.41153   best_test:3.93081\n",
            "epoch 826   loss_test:10.92940   best_test:3.93081\n",
            "epoch 827   loss_test:4.65181   best_test:3.93081\n",
            "epoch 828   loss_test:6.00027   best_test:3.93081\n",
            "epoch 829   loss_test:5.40262   best_test:3.93081\n",
            "epoch 830   loss_test:4.90612   best_test:3.93081\n",
            "epoch 831   loss_test:4.78845   best_test:3.93081\n",
            "epoch 832   loss_test:5.56815   best_test:3.93081\n",
            "epoch 833   loss_test:4.99891   best_test:3.93081\n",
            "epoch 834   loss_test:6.40810   best_test:3.93081\n",
            "epoch 835   loss_test:6.50606   best_test:3.93081\n",
            "epoch 836   loss_test:6.37417   best_test:3.93081\n",
            "epoch 837   loss_test:5.18940   best_test:3.93081\n",
            "epoch 838   loss_test:6.20013   best_test:3.93081\n",
            "epoch 839   loss_test:5.33502   best_test:3.93081\n",
            "epoch 840   loss_test:10.29887   best_test:3.93081\n",
            "epoch 841   loss_test:6.95453   best_test:3.93081\n",
            "epoch 842   loss_test:5.55892   best_test:3.93081\n",
            "epoch 843   loss_test:5.94897   best_test:3.93081\n",
            "epoch 844   loss_test:6.28552   best_test:3.93081\n",
            "epoch 845   loss_test:5.33773   best_test:3.93081\n",
            "epoch 846   loss_test:6.65156   best_test:3.93081\n",
            "epoch 847   loss_test:6.71728   best_test:3.93081\n",
            "epoch 848   loss_test:6.21488   best_test:3.93081\n",
            "epoch 849   loss_test:4.80491   best_test:3.93081\n",
            "epoch 850   loss_test:6.80352   best_test:3.93081\n",
            "epoch 851   loss_test:4.98603   best_test:3.93081\n",
            "epoch 852   loss_test:5.80838   best_test:3.93081\n",
            "epoch 853   loss_test:7.46231   best_test:3.93081\n",
            "epoch 854   loss_test:5.30262   best_test:3.93081\n",
            "epoch 855   loss_test:5.22547   best_test:3.93081\n",
            "epoch 856   loss_test:6.28523   best_test:3.93081\n",
            "epoch 857   loss_test:9.28557   best_test:3.93081\n",
            "epoch 858   loss_test:6.85170   best_test:3.93081\n",
            "epoch 859   loss_test:6.87386   best_test:3.93081\n",
            "epoch 860   loss_test:6.29226   best_test:3.93081\n",
            "epoch 861   loss_test:6.14698   best_test:3.93081\n",
            "epoch 862   loss_test:5.60167   best_test:3.93081\n",
            "epoch 863   loss_test:13.79595   best_test:3.93081\n",
            "epoch 864   loss_test:6.88284   best_test:3.93081\n",
            "epoch 865   loss_test:5.81171   best_test:3.93081\n",
            "epoch 866   loss_test:7.62392   best_test:3.93081\n",
            "epoch 867   loss_test:5.69744   best_test:3.93081\n",
            "epoch 868   loss_test:5.78851   best_test:3.93081\n",
            "epoch 869   loss_test:6.52173   best_test:3.93081\n",
            "epoch 870   loss_test:8.40685   best_test:3.93081\n",
            "epoch 871   loss_test:29.02294   best_test:3.93081\n",
            "epoch 872   loss_test:11.17501   best_test:3.93081\n",
            "epoch 873   loss_test:8.95418   best_test:3.93081\n",
            "epoch 874   loss_test:9.56176   best_test:3.93081\n",
            "epoch 875   loss_test:6.43198   best_test:3.93081\n",
            "epoch 876   loss_test:7.34993   best_test:3.93081\n",
            "epoch 877   loss_test:12.12807   best_test:3.93081\n",
            "epoch 878   loss_test:6.24376   best_test:3.93081\n",
            "epoch 879   loss_test:6.18096   best_test:3.93081\n",
            "epoch 880   loss_test:7.11600   best_test:3.93081\n",
            "epoch 881   loss_test:6.91410   best_test:3.93081\n",
            "epoch 882   loss_test:6.64841   best_test:3.93081\n",
            "epoch 883   loss_test:5.83569   best_test:3.93081\n",
            "epoch 884   loss_test:5.02794   best_test:3.93081\n",
            "epoch 885   loss_test:7.89263   best_test:3.93081\n",
            "epoch 886   loss_test:7.39020   best_test:3.93081\n",
            "epoch 887   loss_test:5.20017   best_test:3.93081\n",
            "epoch 888   loss_test:6.15067   best_test:3.93081\n",
            "epoch 889   loss_test:6.07749   best_test:3.93081\n",
            "epoch 890   loss_test:5.64224   best_test:3.93081\n",
            "epoch 891   loss_test:6.38076   best_test:3.93081\n",
            "epoch 892   loss_test:5.70061   best_test:3.93081\n",
            "epoch 893   loss_test:8.09244   best_test:3.93081\n",
            "epoch 894   loss_test:6.43242   best_test:3.93081\n",
            "epoch 895   loss_test:6.55890   best_test:3.93081\n",
            "epoch 896   loss_test:5.74416   best_test:3.93081\n",
            "epoch 897   loss_test:5.86645   best_test:3.93081\n",
            "epoch 898   loss_test:6.09073   best_test:3.93081\n",
            "epoch 899   loss_test:6.80095   best_test:3.93081\n",
            "epoch 900   loss_test:5.12881   best_test:3.93081\n",
            "epoch 901   loss_test:6.49984   best_test:3.93081\n",
            "epoch 902   loss_test:6.18596   best_test:3.93081\n",
            "epoch 903   loss_test:5.73766   best_test:3.93081\n",
            "epoch 904   loss_test:5.51586   best_test:3.93081\n",
            "epoch 905   loss_test:9.37319   best_test:3.93081\n",
            "epoch 906   loss_test:6.20181   best_test:3.93081\n",
            "epoch 907   loss_test:5.68885   best_test:3.93081\n",
            "epoch 908   loss_test:12.08323   best_test:3.93081\n",
            "epoch 909   loss_test:8.21677   best_test:3.93081\n",
            "epoch 910   loss_test:7.52012   best_test:3.93081\n",
            "epoch 911   loss_test:6.46347   best_test:3.93081\n",
            "epoch 912   loss_test:6.12872   best_test:3.93081\n",
            "epoch 913   loss_test:6.39120   best_test:3.93081\n",
            "epoch 914   loss_test:6.02615   best_test:3.93081\n",
            "epoch 915   loss_test:5.63767   best_test:3.93081\n",
            "epoch 916   loss_test:5.22260   best_test:3.93081\n",
            "epoch 917   loss_test:5.72263   best_test:3.93081\n",
            "epoch 918   loss_test:4.96755   best_test:3.93081\n",
            "epoch 919   loss_test:5.39996   best_test:3.93081\n",
            "epoch 920   loss_test:5.60448   best_test:3.93081\n",
            "epoch 921   loss_test:5.87859   best_test:3.93081\n",
            "epoch 922   loss_test:6.57897   best_test:3.93081\n",
            "epoch 923   loss_test:5.38058   best_test:3.93081\n",
            "epoch 924   loss_test:5.66113   best_test:3.93081\n",
            "epoch 925   loss_test:4.70422   best_test:3.93081\n",
            "epoch 926   loss_test:7.09379   best_test:3.93081\n",
            "epoch 927   loss_test:5.64623   best_test:3.93081\n",
            "epoch 928   loss_test:6.81092   best_test:3.93081\n",
            "epoch 929   loss_test:6.95927   best_test:3.93081\n",
            "epoch 930   loss_test:11.40043   best_test:3.93081\n",
            "epoch 931   loss_test:5.19666   best_test:3.93081\n",
            "epoch 932   loss_test:5.14238   best_test:3.93081\n",
            "epoch 933   loss_test:5.11803   best_test:3.93081\n",
            "epoch 934   loss_test:4.87849   best_test:3.93081\n",
            "epoch 935   loss_test:5.54429   best_test:3.93081\n",
            "epoch 936   loss_test:5.16206   best_test:3.93081\n",
            "epoch 937   loss_test:26.25773   best_test:3.93081\n",
            "epoch 938   loss_test:6.89200   best_test:3.93081\n",
            "epoch 939   loss_test:5.44011   best_test:3.93081\n",
            "epoch 940   loss_test:5.50044   best_test:3.93081\n",
            "epoch 941   loss_test:5.04628   best_test:3.93081\n",
            "epoch 942   loss_test:5.09827   best_test:3.93081\n",
            "epoch 943   loss_test:6.96325   best_test:3.93081\n",
            "epoch 944   loss_test:7.03588   best_test:3.93081\n",
            "epoch 945   loss_test:10.64123   best_test:3.93081\n",
            "epoch 946   loss_test:7.50050   best_test:3.93081\n",
            "epoch 947   loss_test:6.59914   best_test:3.93081\n",
            "epoch 948   loss_test:6.41257   best_test:3.93081\n",
            "epoch 949   loss_test:7.39360   best_test:3.93081\n",
            "epoch 950   loss_test:6.16685   best_test:3.93081\n",
            "epoch 951   loss_test:5.84010   best_test:3.93081\n",
            "epoch 952   loss_test:7.16025   best_test:3.93081\n",
            "epoch 953   loss_test:6.15932   best_test:3.93081\n",
            "epoch 954   loss_test:5.83448   best_test:3.93081\n",
            "epoch 955   loss_test:6.56145   best_test:3.93081\n",
            "epoch 956   loss_test:5.29106   best_test:3.93081\n",
            "epoch 957   loss_test:6.44683   best_test:3.93081\n",
            "epoch 958   loss_test:6.38319   best_test:3.93081\n",
            "epoch 959   loss_test:7.25921   best_test:3.93081\n",
            "epoch 960   loss_test:6.12261   best_test:3.93081\n",
            "epoch 961   loss_test:11.02009   best_test:3.93081\n",
            "epoch 962   loss_test:4.89025   best_test:3.93081\n",
            "epoch 963   loss_test:7.97582   best_test:3.93081\n",
            "epoch 964   loss_test:6.56678   best_test:3.93081\n",
            "epoch 965   loss_test:7.43520   best_test:3.93081\n",
            "epoch 966   loss_test:6.77282   best_test:3.93081\n",
            "epoch 967   loss_test:5.70369   best_test:3.93081\n",
            "epoch 968   loss_test:5.88709   best_test:3.93081\n",
            "epoch 969   loss_test:6.48647   best_test:3.93081\n",
            "epoch 970   loss_test:5.83680   best_test:3.93081\n",
            "epoch 971   loss_test:5.94043   best_test:3.93081\n",
            "epoch 972   loss_test:5.66819   best_test:3.93081\n",
            "epoch 973   loss_test:7.02530   best_test:3.93081\n",
            "epoch 974   loss_test:5.52652   best_test:3.93081\n",
            "epoch 975   loss_test:5.72533   best_test:3.93081\n",
            "epoch 976   loss_test:4.65673   best_test:3.93081\n",
            "epoch 977   loss_test:5.40517   best_test:3.93081\n",
            "epoch 978   loss_test:11.94336   best_test:3.93081\n",
            "epoch 979   loss_test:6.68737   best_test:3.93081\n",
            "epoch 980   loss_test:6.43875   best_test:3.93081\n",
            "epoch 981   loss_test:5.63430   best_test:3.93081\n",
            "epoch 982   loss_test:7.70618   best_test:3.93081\n",
            "epoch 983   loss_test:7.35028   best_test:3.93081\n",
            "epoch 984   loss_test:7.68401   best_test:3.93081\n",
            "epoch 985   loss_test:7.19639   best_test:3.93081\n",
            "epoch 986   loss_test:5.34796   best_test:3.93081\n",
            "epoch 987   loss_test:5.35666   best_test:3.93081\n",
            "epoch 988   loss_test:4.89111   best_test:3.93081\n",
            "epoch 989   loss_test:5.63622   best_test:3.93081\n",
            "epoch 990   loss_test:10.79670   best_test:3.93081\n",
            "epoch 991   loss_test:6.05699   best_test:3.93081\n",
            "epoch 992   loss_test:7.75894   best_test:3.93081\n",
            "epoch 993   loss_test:6.44019   best_test:3.93081\n",
            "epoch 994   loss_test:5.85626   best_test:3.93081\n",
            "epoch 995   loss_test:11.98209   best_test:3.93081\n",
            "epoch 996   loss_test:6.40629   best_test:3.93081\n",
            "epoch 997   loss_test:9.27100   best_test:3.93081\n",
            "epoch 998   loss_test:5.88690   best_test:3.93081\n",
            "epoch 999   loss_test:5.48071   best_test:3.93081\n",
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASY0lEQVR4nO3dedBddX3H8feXoIBEQpSAC9QHCMIAgyCp+5JohVAJbqik1kFkSNuRLraOSl1Qq9WpjHXcjYA7RIZRB5QKOBLQStVEhYKARoQWxLKIgQQEhW//OCc/Hx7vvblPnpx77nnyfs3cec5yz30+OYN+nrPc34nMRJIkgO3aDiBJGh+WgiSpsBQkSYWlIEkqLAVJUrF92wFmYrfddsuJiYm2YzzExo0b2XnnnduOMZQuZYVu5e1SVuhW3i5lhfHMu3bt2tszc0GvdZ0uhYmJCdasWdN2jIdYvXo1ixcvbjvGULqUFbqVt0tZoVt5u5QVxjNvRNzYb52njyRJhaUgSSosBUlSYSlIkgpLQZJUjE0pRMQ+EXFGRJzbdhZJ2lY1WgoRcWZE3BoRV01ZvjQirouIdRHxZoDMvD4zT2wyjyRpsKaPFD4DLJ28ICLmAB8FjgIOBJZHxIEN55AkDSGafp5CREwAX8vMg+v5pwPvyMwj6/lTADLzvfX8uZl57IDPWwGsANhjjz0OX7VqVaP5p2vDhg3MnTu37RhD6VJW6FbeLmWFbuXtUlYYz7xLlixZm5mLeq7MzEZfwARw1aT5Y4HTJ82/GvgI8GjgE8DPgVOG+ezDDz88x80ll1zSdoShdSlrZrfydilrZrfydilr5njmBdZkn/9fHZthLjLzDuCv284hSduyNu4+uhnYa9L8nvUySVLL2iiFHwD7RcTeEfFw4DjgvBZySJKmaPqW1LOBy4H9I+KmiDgxM38PnAxcCFwDnJOZVzeZQ5I0nEavKWTm8j7LLwAuaPJ3S5Kmb2y+0SxJap+lIEkqOlkKEbEsIlauX7++7SiSNKt0shQy8/zMXDFv3ry2o0jSrNLJUpAkNcNSkCQVloIkqbAUJEmFpSBJKiwFSVJhKUiSCktBklR0shT8RrMkNaOTpeA3miWpGZ0sBUlSMywFSVJhKUiSCktBklRYCpKkwlKQJBWWgiSpsBQkSYWlIEkqLAVJUtHJUnDsI0lqRidLwbGPJKkZnSwFSVIzLAVJUmEpSJIKS0GSVFgKkqTCUpAkFZaCJKmwFCRJhaUgSSosBUlSYSlIkopOloID4klSMzpZCg6IJ0nN6GQpSJKaYSlIkgpLQZJUWAqSpMJSkCQVloIkqbAUJEmFpSBJKiwFSVJhKUiSCktBklRYCpKkwlKQJBWWgiSp6GQp+DwFSWpGJ0vB5ylIUjM6WQqSpGZYCpKkwlKQJBWWgiSpsBQkSYWlIEkqLAVJUtG3FCLijZOmXz5l3b82GUqS1I5BRwrHTZo+Zcq6pQ1kkSS1bFApRJ/pXvOSpFlgUClkn+le85KkWWD7AeueFBF3UR0V7FRPU8/v2HgySdLI9S2FzJwzyiCSpPb1LYWIeATwu8z8XT2/P/DnwA2Z+ZUR5ZMkjdCgawrfACYAImIhcDmwD3ByRLyv+WiSpFEbVArzM/Nn9fTxwNmZ+bfAUcALG08mSRq5Ye8+eh5wMUBm3g882GQoSVI7Bt19dGVEnAbcDCwELgKIiF1HEWyQiFgGLFu4cGHbUSRpVhl0pHAScDvVdYUjMvOeevmBwGkN5xrIx3FKUjMG3ZJ6L/BHF5Qz87vAd5sMJUlqx6BbUq8ctGFmHrL140iS2jTomsKDVBebzwLOB+4dSSJJUmv6XlPIzEOB5cBcqmJ4D3AQcHNm3jiaeJKkURr4kJ3MvDYzT83MJ1MdLXwOeP1IkkmSRm7Q6SMi4vFUz1V4CXAnVSE4xIUkzVKDLjRfCjwSOAc4AbijXvXwiHhUZv56BPkkSSM06EjhCVQXmv8KWDFpedTL92kwlySpBYO+pzAxwhySpDEw8EKzJGnbYilIkgpLQZJUDFUKEfGsiDihnl4QEXs3G0uS1IbNlkJEnAq8CTilXvQw4AtNhpIktWOYI4WXAMcAGwEy85dU31+QJM0yw5TC/ZmZ1E9ii4idm40kSWrLMKVwTkR8Etg1Ik4Cvgl8qtlYkqQ2DBz7CCAzT4uIFwB3AfsDb8/MixtPJkkauc2WAkBdAhaBJM1ymy2FiLib+nrCJOuBNcA/Zeb1TQSTJI3eMEcKHwRuonrQTlANpb0v8EPgTGBxU+EkSaM1zIXmYzLzk5l5d2belZkrgSMz80vA/IbzSZJGaJhSuCciXhER29WvVwC/rddNPa0kSeqwYUrhVcCrgVuB/6un/zIidgJObjBbXxGxLCJWrl+/vo1fL0mz1mZLITOvz8xlmblbZi6op9dl5r2Z+Z1RhOyR6fzMXDFv3rw2fr0kzVrD3H20I3AicBCw46blmfnaBnNJklowzOmjzwOPAY4ELgX2BO5uMpQkqR3DlMLCzHwbsDEzPwu8EHhqs7EkSW0YphR+V//8TUQcDMwDdm8ukiSpLcN8eW1lRMwH3gqcB8wF3tZoKklSKwaWQkRsB9yVmXcClwH7jCSVJKkVA08fZeaDwBtHlEWS1LJhril8MyLeEBF7RcSjNr0aTyZJGrlhrim8sv75uknLEk8lSdKsM8xDdvYeRRBJUvs2e/ooIh4REW+NiJX1/H4RcXTz0SRJozbMNYVPA/cDz6jnbwbe3VgiSVJrhimFfTPz36i/xJaZ91A9bEeSNMsMUwr318NkJ0BE7Avc12gqSVIrhrn76B3AN4C9IuKLwDOB1zSYSZLUkmHuProoItYCT6M6bfT3mXl748kkSSM3zPMUzgfOAs7LzI3NR5IktWWYawqnAc8GfhIR50bEsfWDdyRJs8wwp48uBS6NiDnA84CTgDOBXRrOJkkasWEuNFPffbSMasiLJwOfbTKUJKkdw1xTOAd4CtUdSB8BLq1HT5UkzTLDHCmcASzPzAcAIuJZEbE8M1+3me0kSR0zzDWFCyPisIhYDrwC+AXw5caTSZJGrm8pRMQTgeX163bgS0Bk5pIRZZMkjdigI4VrgW8DR2fmOoCIeP1IUkmSWjHoewovBW4BLomIT0XE83EgPEma1fqWQmZ+NTOPAw4ALgH+Adg9Ij4eEUeMKqAkaXQ2+43mzNyYmWdl5jJgT+BHwJsaTyZJGrlhhrkoMvPOzFyZmc9vKpAkqT3TKgVJ0uxmKUiSCktBklR0shQiYllErFy/fn3bUSRpVulkKWTm+Zm5Yt68eW1HkaRZpZOlIElqhqUgSSosBUlSYSlIkgpLQZJUWAqSpMJSkCQVloIkqbAUJEmFpSBJKiwFSVJhKUiSCktBklRYCpKkwlKQJBWWgiSpsBQkSYWlIEkqLAVJUmEpSJIKS0GSVFgKkqTCUpAkFZaCJKmwFCRJhaUgSSosBUlSYSlIkgpLQZJUWAqSpMJSkCQVloIkqbAUJEmFpSBJKiwFSVJhKUiSCktBklRYCpKkwlKQJBWWgiSpsBQkSYWlIEkqLAVJUmEpSJIKS0GSVFgKkqTCUpAkFZaCJKmwFCRJhaUgSSosBUlSYSlIkgpLQZJUWAqSpMJSkCQV27cdYJOI2Bn4GHA/sDozv9hyJEna5jR6pBARZ0bErRFx1ZTlSyPiuohYFxFvrhe/FDg3M08CjmkylySpt6ZPH30GWDp5QUTMAT4KHAUcCCyPiAOBPYH/rd/2QMO5JEk9RGY2+wsiJoCvZebB9fzTgXdk5pH1/Cn1W28C7szMr0XEqsw8rs/nrQBWAOyxxx6Hr1q1qtH807Vhwwbmzp3bdoyhdCkrdCtvl7JCt/J2KSuMZ94lS5aszcxFvda1cU3h8fzhiACqMngq8CHgIxHxQuD8fhtn5kpgJcCiRYty8eLFzSXdAqtXr2bcMvXTpazQrbxdygrdytulrNC9vGNzoTkzNwIntJ1DkrZlbdySejOw16T5PetlkqSWtVEKPwD2i4i9I+LhwHHAeS3kkCRN0fQtqWcDlwP7R8RNEXFiZv4eOBm4ELgGOCczr24yhyRpOI1eU8jM5X2WXwBc0OTvliRNn8NcSJIKS0GSVHSyFCJiWUSsXL9+fdtRJGlWafwbzU2KiNuAG9vOMcVuwO1thxhSl7JCt/J2KSt0K2+XssJ45n1CZi7otaLTpTCOImJNv6+Pj5suZYVu5e1SVuhW3i5lhe7l7eTpI0lSMywFSVJhKWx9K9sOMA1dygrdytulrNCtvF3KCh3L6zUFSVLhkYIkqbAUJEmFpTCkiNgxIr4fEVdExNUR8c56+Rn1sisj4tyI+KNHLEXERETcGxE/rl+faCPrpPUfiogNA7Y/pX5+9nURcWSTWWead1z2bUR8JiJ+MSnHoX22Pz4ifla/jm8y61bK+8Ck9zQ6mvGArBER74mIn0bENRHxd322H9m+3QpZR7Zfpy0zfQ3xAgKYW08/DPge8DRgl0nv+QDw5h7bTgBXtZ21nl8EfB7Y0GfbA4ErgB2AvYGfA3PGOO9Y7Fuq55Efu5ltHwVcX/+cX0/PH9e89TY99/uIs54AfA7Yrl63e9v7diZZR71fp/vySGFIWdn01+rD6ldm5l1Q/YUA7AS0fuW+X9aImAO8H3jjgM1fBKzKzPsy8xfAOuApY5x3pPplHXLzI4GLM/PXmXkncDGwtIGYxQzzjtSArH8DvCszH6zfd2uPzUe6b2eYdaxZCtMQEXMi4sfArVT/AX6vXv5p4FfAAcCH+2y+d0T8KCIujYhnt5T1ZOC8zLxlwKa9nqH9+OaSVmaQF8Zj3wK8pz6N+O8RsUOPTcdp38Lm8wLsGBFrIuK/IuLFLWXdF3hlneM/ImK/HpuOfN/OICuMeL9Oh6UwDZn5QGYeSvUI0adExMH18hOAx1E9NOiVPTa9BfiTzDwM+EfgrIjYZcRZnwO8nP6l1aoZ5B2HfXswcArVHwV/SnUK401NZpiOGeZ9QlZDNPwF8MGI2LeFrDsAv61zfAo4s8kMw5ph1pHu1+mwFLZAZv4GuIRJh6eZ+QCwCnhZj/ffl5l31NNrqc7TP3HEWZcAC4F1EXED8IiIWNdjk1afoT3dvGOyb5dm5i31KYX7gE/T+5TbuOzbYfOSmTfXP68HVgOHjTor1V/9X65XfQU4pMcmre3bLcja2n4dhqUwpIhYEBG71tM7AS8ArouIhfWyAI4Bru2z7Zx6eh9gP6oLYaPMujYzH5OZE5k5AdyTmQt7bH4ecFxE7BARe9dZv99U1pnmHZN9e21EPLZeFsCLgat6bH4hcEREzI+I+cAR9bLGzCRvnXOHeno34JnAT0adFfgq1R8JAM8Fftpj85Hu25lkHfV+nbbpXJXell9Ujf8j4Eqq/wG9napU/xP473rZF6nvRqIqiHfV0y8DrgZ+DPwQWDbqrD3es2HSdMlaz7+F6i/u64Cj2ti3w+Ydl30LfGvSfwdf4A93piwCTp+0/WupLt6vA05oa98Okxd4Rv2eK+qfJ7aUdVfg63WGy4Entb1vZ5J11Pt1ui+HuZAkFZ4+kiQVloIkqbAUJEmFpSBJKiwFSVJhKWibFxFvqUe6vLIetfKp9fLVEbFm0vsWRcTqenpxRKyv339tRJw24PMPi4gz+qy7ob5XffLImVdExA8j4hn18gUR8Y2t+E+W+rIUtE2LiKcDRwNPzsxDgD/joWPo7B4RR/XZ/NtZDXNwGHB0RDyzz/v+GfjQEHHuzcxDM/NJVMNQvBcgM28Dbhnw+dJWYyloW/dY4PashnsgM2/PzF9OWv9+qi/z9ZWZ91J9ee6PBmCLiEcCh2TmFfX8oyPiovrI5HSqIZh72QW4c9L8V4FXDflvkraYpaBt3UXAXlE9FOVjEfHcKesvB+6PiCU9tgWqYQuohte4rMfqRTx0CIlTge9k5kFUY+P8yaR1O206HQWcDvzLpHVrgMZHgJUsBW3TshoT/3BgBXAb8KWIeM2Ut70beGuPzZ8dEVdQDbx2YWb+qsd7Hlt/7ibPoRpWgsz8Og89Gth0+ugAqsHVPlePTQTV8MyPm86/TdoSloK2eVkNgbw6M0+leobDy6as/xbVA5SeNmXTb9fn/w8CTozej7S8F9hxCzJdDuwGLKgX7Vh/ltQoS0HbtIjYf8qDUA4Fbuzx1nfT5wlwWT2h7n30fibBNVRDgG9yGdUY+tQXsOf3yXUAMAe4o170RHqPvCptVdu3HUBq2Vzgw/UwyL+nGmFzxdQ3ZeYFEXHb1OWTfAJ4Q0RMZOYNk7a7NiLmRcQjM/Nu4J3A2RFxNfBd4H8mfcZO9ZO8oLoAfXxWz+mAajjmr2/ZP1EanqOkSg2LiNcDd2fm6TP4jMuAF2X1/GGpMZ4+kpr3ceC+Ld04IhYAH7AQNAoeKUiSCo8UJEmFpSBJKiwFSVJhKUiSCktBklT8P6v82dEMEHZ7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import keras.layers.normalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers import Dense\n",
        "#from wsr.bcd.generate_received_pilots import generate_received_pilots_batch\n",
        "\n",
        "\n",
        "# these three lines are needed if using google colab, otherwise can delete\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = '/content/drive/MyDrive/'\n",
        "\n",
        "'System Information'\n",
        "N = 1   #Number of BS's antennas\n",
        "delta_inv = 128 #Number of posterior intervals inputed to DNN \n",
        "delta = 1/delta_inv \n",
        "S = np.log2(delta_inv) \n",
        "OS_rate = 20 #Over sampling rate in each AoA interval (N_s in the paper)\n",
        "delta_inv_OS = OS_rate*delta_inv #Total number of AoAs for posterior computation\n",
        "delta_OS = 1/delta_inv_OS \n",
        "'Channel Information'\n",
        "phi_min = -60*(np.pi/180) #Lower-bound of AoAs\n",
        "phi_max = 60*(np.pi/180) #Upper-bound of AoAs\n",
        "num_SNR = 8 #Number of considered SNRs\n",
        "\n",
        "  #  x = np.random.uniform(5, 55) \n",
        "  #  y = np.random.uniform(-35, 35)\n",
        "\n",
        "location_bs_new = np.array([40, -40, 0])\n",
        "tau =  6 #Pilot length\n",
        "\n",
        "snr_const = 35 #The SNR\n",
        "snr_const = np.array([snr_const])\n",
        "Pvec = 10**(snr_const/10) #Set of considered TX powers\n",
        "\n",
        "\n",
        "mean_true_alpha = 0.0 + 0.0j #Mean of the fading coefficient\n",
        "std_per_dim_alpha = np.sqrt(0.5) #STD of the Gaussian fading coefficient per real dim.\n",
        "noiseSTD_per_dim = np.sqrt(0.5) #STD of the Gaussian noise per real dim.\n",
        "#####################################################\n",
        "'RIS'\n",
        "N_ris = 64\n",
        "num_users = 1\n",
        "params_system = (N,N_ris,num_users)\n",
        "Rician_factor = 10\n",
        "location_user = None\n",
        "\n",
        "#####################################################\n",
        "'Learning Parameters'\n",
        "initial_run = 0    #0: Continue training; 1: Starts from the scratch\n",
        "if initial_run == 1:\n",
        "  print('!!!!! training from scratch !!!!!')\n",
        "n_epochs = 1000 #Num of epochs\n",
        "learning_rate = 0.00005 #Learning rate\n",
        "batch_per_epoch = 100 #Number of mini batches per epoch\n",
        "batch_size_order = 8 #Mini_batch_size = batch_size_order*delta_inv\n",
        "val_size_order = 782 #Validation_set_size = val_size_order*delta_inv\n",
        "scale_factor = 1 #Scaling the number of tests\n",
        "test_size_order = 782 #Test_set_size = test_size_order*delta_inv*scale_factor\n",
        "######################################################\n",
        "tf.reset_default_graph() #Reseting the graph\n",
        "he_init = tf.variance_scaling_initializer() #Define initialization method\n",
        "######################################## Place Holders\n",
        "#alpha_input = tf.placeholder(tf.float32, shape=(None,1), name=\"alpha_input\")\n",
        "loc_input = tf.placeholder(tf.float32, shape=(None,1,3), name=\"loc_input\")\n",
        "channel_bs_irs_user = tf.placeholder(tf.float32, shape=(None, 2 * N_ris, 2 * N, num_users), name=\"channel_bs_irs_user\")\n",
        "channel_bs_user = tf.placeholder(tf.float32, shape=(None, 2 * N, num_users), name=\"channel_bs_irs_user\")\n",
        "######################################################\n",
        "#Constructing the array responses for AoA samples\n",
        "##################### NETWORK\n",
        "with tf.name_scope(\"array_response_construction\"):\n",
        "    lay = {}\n",
        "    lay['P'] = tf.constant(1.0)\n",
        "    ###############\n",
        "    from0toN = tf.cast(tf.range(0, N, 1),tf.float32)\n",
        "    \n",
        "with tf.name_scope(\"channel_sensing\"):\n",
        "    hidden_size = 512\n",
        "    A1 = tf.get_variable(\"A1\",  shape=[hidden_size,1024], dtype=tf.float32, initializer= he_init)\n",
        "    A2 = tf.get_variable(\"A2\",  shape=[1024,1024], dtype=tf.float32, initializer= he_init)\n",
        "    A3 = tf.get_variable(\"A3\",  shape=[1024,1024], dtype=tf.float32, initializer= he_init)\n",
        "    A4 = tf.get_variable(\"A4\",  shape=[1024,2*N_ris], dtype=tf.float32, initializer= he_init)\n",
        "    \n",
        "    b1 = tf.get_variable(\"b1\",  shape=[1024], dtype=tf.float32, initializer= he_init)\n",
        "    b2 = tf.get_variable(\"b2\",  shape=[1024], dtype=tf.float32, initializer= he_init)\n",
        "    b3 = tf.get_variable(\"b3\",  shape=[1024], dtype=tf.float32, initializer= he_init)\n",
        "    b4 = tf.get_variable(\"b4\",  shape=[2*N_ris], dtype=tf.float32, initializer= he_init)\n",
        "        \n",
        "    w_dict = []\n",
        "    posterior_dict = []\n",
        "    idx_est_dict = []\n",
        "    layer_Ui = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Wi = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Uf = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Wf = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Uo = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Wo = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Uc = Dense(units=hidden_size, activation='linear')\n",
        "    layer_Wc = Dense(units=hidden_size, activation='linear')\n",
        "    def RNN(input_x, h_old, c_old):\n",
        "        i_t = tf.sigmoid(layer_Ui(input_x) + layer_Wi(h_old))\n",
        "        f_t = tf.sigmoid(layer_Uf(input_x) + layer_Wf(h_old))\n",
        "        o_t = tf.sigmoid(layer_Uo(input_x) + layer_Wo(h_old))\n",
        "        c_t = tf.tanh(layer_Uc(input_x) + layer_Wc(h_old))\n",
        "        c = i_t * c_t + f_t * c_old     # cell state\n",
        "        h_new = o_t * tf.tanh(c)        # hidden state\n",
        "        return h_new, c\n",
        "    \n",
        "    snr = lay['P']*tf.ones(shape=[tf.shape(loc_input)[0],1],dtype=tf.float32)\n",
        "    snr_dB = tf.log(snr)/np.log(10)\n",
        "    snr_normal = (snr_dB-1)/np.sqrt(1.6666) #Normalizing for the range -10dB to 30dB\n",
        "    \n",
        "    for t in range(tau):      \n",
        "        'DNN designs the next sensing direction'\n",
        "        if t == 0:\n",
        "            RSS_val = tf.ones([tf.shape(loc_input)[0],1])\n",
        "            h_old = tf.zeros([tf.shape(loc_input)[0],hidden_size])\n",
        "            c_old = tf.zeros([tf.shape(loc_input)[0],hidden_size])\n",
        "        h_old, c_old = RNN(tf.concat([RSS_val,snr_normal],axis=1), h_old, c_old)\n",
        "\n",
        "        x1 = tf.nn.relu(h_old@A1+b1)\n",
        "        x1 = BatchNormalization()(x1)\n",
        "        x2 = tf.nn.relu(x1@A2+b2)\n",
        "        x2 = BatchNormalization()(x2)\n",
        "        x3 = tf.nn.relu(x2@A3+b3)\n",
        "        x3 = BatchNormalization()(x3)\n",
        "        '''\n",
        "            RIS implementation\n",
        "        '''\n",
        "        ris_her_unnorm = x3 @ A4 + b4\n",
        "        ris_her_r = ris_her_unnorm[:, 0:N_ris]\n",
        "        ris_her_i = ris_her_unnorm[:, N_ris:2 * N_ris]                      # (? , N_ris)\n",
        "        theta_tmp = tf.sqrt(tf.square(ris_her_r) + tf.square(ris_her_i))    # (? , N_ris)\n",
        "        theta_real = ris_her_r / theta_tmp                                  # (? , N_ris)\n",
        "        theta_imag = ris_her_i / theta_tmp                                  # (? , N_ris)\n",
        "        theta = tf.concat([theta_real, theta_imag], axis=1)                 # (? , 2*N_ris)      hmmmmm\n",
        "        #print('theta:', theta.shape)\n",
        "        theta_T = tf.reshape(theta, [-1, 1, 2 * N_ris])                     # (? , 1 , 2 * N_ris)\n",
        "        #print('theta_T:', theta_T.shape)                                   #  (?, 1, 128)\n",
        "\n",
        "        'BS observes the next measurement'\n",
        "        \n",
        "        A_T_k = channel_bs_irs_user[:, :, :, 0] # since 1 user\n",
        "        theta_A_k_T = tf.matmul(theta_T, A_T_k)                             # (? , 1 , 2 * N ) \n",
        "\n",
        "        h_d = channel_bs_user[:,:,0]\n",
        "        h_d_T = tf.reshape(h_d, [-1, 1, 2 * N])\n",
        "\n",
        "        h_d_plus_h_cas = h_d_T + theta_A_k_T\n",
        "        h_d_plus_h_cas_re = h_d_plus_h_cas[:,:,0]\n",
        "        h_d_plus_h_cas_im = h_d_plus_h_cas[:,:,1]\n",
        "        noise =  tf.complex(tf.random_normal(tf.shape(h_d_plus_h_cas_re), mean = 0.0, stddev = noiseSTD_per_dim),\\\n",
        "                    tf.random_normal(tf.shape(h_d_plus_h_cas_re), mean = 0.0, stddev = noiseSTD_per_dim))\n",
        "        y_complex = tf.complex(tf.sqrt(lay['P']),0.0)*tf.complex(h_d_plus_h_cas_re,h_d_plus_h_cas_im) + noise\n",
        "        y_real = tf.concat([tf.real(y_complex),tf.imag(y_complex)],axis=1)/tf.sqrt(lay['P'])\n",
        "        RSS_val = tf.math.square(tf.abs(y_complex))\n",
        "\n",
        "    h_old, c_old = RNN(tf.concat([RSS_val,snr_normal],axis=1), h_old, c_old)\n",
        "    loc_hat = Dense(units=3, activation='linear')(c_old)\n",
        "\n",
        "####################################################################################\n",
        "####### Loss Function\n",
        "a = tf.math.reduce_euclidean_norm(loc_input[:,0,:]-loc_input[:,0,:], 1)\n",
        "b = tf.math.reduce_euclidean_norm(loc_hat-loc_input[:,0,:], 1)\n",
        "loss = tf.keras.losses.mean_squared_error(a, b)\n",
        "print(loss)\n",
        "####### Optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "#########################################################################\n",
        "###########  Validation Set\n",
        "channel_true_val, set_location_user_val = generate_channel_fullRician(params_system, location_bs = location_bs_new,\n",
        "                                                        num_samples=val_size_order*delta_inv, location_user_initial=location_user, Rician_factor=Rician_factor)\n",
        "(channel_bs_user_val, channel_irs_user_val, channel_bs_irs_val) = channel_true_val\n",
        "A_T_real_val, Hd_real_val , channel_bs_irs_user_val = channel_complex2real(channel_true_val)\n",
        "feed_dict_val = {loc_input: np.array(set_location_user_val),\n",
        "                    channel_bs_irs_user: A_T_real_val,\n",
        "                    channel_bs_user : Hd_real_val,\n",
        "                    lay['P']: Pvec[0]}\n",
        "###########  Training\n",
        "with tf.Session() as sess:\n",
        "    if initial_run == 1:\n",
        "        init.run()\n",
        "    else:\n",
        "        saver.restore(sess, drive_save_path + '/params_new_closeBS_fullRician_RSS_3D_RIS_tau_'+ str(tau) +'_snr_'+ str(snr_const[0]))\n",
        "    best_loss, pp = sess.run([loss,posterior_dict], feed_dict=feed_dict_val)\n",
        "    print(best_loss)\n",
        "    print(tf.test.is_gpu_available()) #Prints whether or not GPU is on\n",
        "    for epoch in range(n_epochs):\n",
        "        batch_iter = 0\n",
        "        for rnd_indices in range(batch_per_epoch):\n",
        "\n",
        "            snr_temp = snr_const[0]\n",
        "            P_temp = 10**(snr_temp/10)\n",
        "            \n",
        "            '''\n",
        "                RIS implementation\n",
        "            '''\n",
        "            channel_true_train, set_location_user_train = generate_channel_fullRician(params_system, location_bs = location_bs_new,\n",
        "                                                        num_samples=batch_size_order*delta_inv,\n",
        "                                                       location_user_initial=location_user, Rician_factor=Rician_factor)\n",
        "            (channel_bs_user_train, channel_irs_user_train, channel_bs_irs_train) = channel_true_train\n",
        "            A_T_real, Hd_real_train , channel_bs_irs_user_train = channel_complex2real(channel_true_train)\n",
        "            #print(A_T_real.shape)\n",
        "            #print(channel_bs_irs_user_train.shape)\n",
        "            #print(channel_bs_user_train.shape) #(num_samples,1,1)\n",
        "            #print(channel_irs_user_train.shape) # (num_samples,64,1)\n",
        "            #print(channel_bs_irs_train.shape) # (num_samples,1,64)\n",
        "            #print(aoa_irs_y_set_train.shape) # this is what we wanna estimate, shape: (num_sample,1)\n",
        "            #print(pathloss_irs_user_set_train.shape)  # this is known  shape  (num_sample, )\n",
        "            feed_dict_batch = {loc_input: np.array(set_location_user_train),\n",
        "                              channel_bs_irs_user: A_T_real,\n",
        "                               channel_bs_user : Hd_real_train,\n",
        "                              lay['P']: P_temp}\n",
        "            sess.run(training_op, feed_dict=feed_dict_batch)\n",
        "            batch_iter += 1\n",
        "\n",
        "        \n",
        "        loss_val = sess.run(loss, feed_dict=feed_dict_val)\n",
        "        print('epoch',epoch,'  loss_test:%2.5f'%loss_val,'  best_test:%2.5f'%best_loss) \n",
        "        #print('epoch',epoch,'  loss_test:',loss_val, 'best_test:',best_loss) \n",
        "        if epoch%2 == 1: #Every 10 iterations it checks if the validation performace is improved, then saves parameters\n",
        "            if loss_val < best_loss:\n",
        "                save_path = saver.save(sess, drive_save_path+'/params_new_closeBS_fullRician_RSS_3D_RIS_tau_'+ str(tau) +'_snr_'+ str(snr_const[0]))\n",
        "                best_loss = loss_val\n",
        "\n",
        "###########  Final Test    \n",
        "    performance = np.zeros([1,scale_factor])\n",
        "    for j in range(scale_factor):\n",
        "        print(j)\n",
        "       \n",
        "        channel_true_test, set_location_user_test = generate_channel_fullRician(params_system, location_bs = location_bs_new,\n",
        "                                                        num_samples=test_size_order*delta_inv,\n",
        "                                                       location_user_initial=location_user, Rician_factor=Rician_factor)\n",
        "        (channel_bs_user_test, channel_irs_user_test, channel_bs_irs_test) = channel_true_test\n",
        "        A_T_real_test, Hd_real_test , channel_bs_irs_user_test = channel_complex2real(channel_true_test)\n",
        "        \n",
        "        feed_dict_test = {loc_input: np.array(set_location_user_test),\n",
        "                            channel_bs_irs_user: A_T_real_test,\n",
        "                            channel_bs_user : Hd_real_test,\n",
        "                            lay['P']: Pvec[0]}\n",
        "            \n",
        "        mse_loss,phi_hat_test= sess.run([loss,loc_hat],feed_dict=feed_dict_test)\n",
        "        performance[0,j] = mse_loss\n",
        "            \n",
        "    performance = np.mean(performance,axis=1)       \n",
        "            \n",
        "######### Plot the test result /params_unknownalpha_RIS_snr_'+ str(snr_const[0]))\n",
        "plt.semilogy(snr_const, performance)        \n",
        "plt.grid()\n",
        "plt.xlabel('SNR (dB)')\n",
        "plt.ylabel('Average MSE')\n",
        "plt.show()\n",
        "sio.savemat(drive_save_path+'/data_RNN_loc_closeBS_fullRician_RSS_3D_RIS_tau_'+ str(tau) +'_snr_'+ str(snr_const[0]) +'.mat',dict(performance= performance,\\\n",
        "                                       snr_const=snr_const,N=N,N_ris = N_ris,epoch = n_epochs,delta_inv=delta_inv,\\\n",
        "                                       mean_true_alpha=mean_true_alpha,\\\n",
        "                                       std_per_dim_alpha=std_per_dim_alpha,\\\n",
        "                                       noiseSTD_per_dim=noiseSTD_per_dim, tau=tau))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}